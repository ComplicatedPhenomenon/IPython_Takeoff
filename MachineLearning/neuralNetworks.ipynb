{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-14T14:01:35.386181Z",
     "start_time": "2019-12-14T14:01:35.376974Z"
    }
   },
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-14T14:01:35.779848Z",
     "start_time": "2019-12-14T14:01:35.616310Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-14T14:01:36.026698Z",
     "start_time": "2019-12-14T14:01:35.786209Z"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-14T14:01:36.036334Z",
     "start_time": "2019-12-14T14:01:36.029415Z"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-13T13:13:00.931433Z",
     "start_time": "2019-12-13T13:13:00.828186Z"
    }
   },
   "source": [
    "https://victorzhou.com/blog/intro-to-neural-networks/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# original approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://victorzhou.com/27cf280166d7159c0465a58c68f99b39/network3.svg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-14T14:29:20.383785Z",
     "start_time": "2019-12-14T14:29:19.993110Z"
    }
   },
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    # Sigmoid activation function: f(x) = 1 / (1 + e^(-x))\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "\n",
    "def deriv_sigmoid(x):\n",
    "    # Derivative of sigmoid: f'(x) = f(x) * (1 - f(x))\n",
    "    fx = sigmoid(x)\n",
    "    return fx * (1 - fx)\n",
    "\n",
    "\n",
    "def mse_loss(y_true, y_pred):\n",
    "    # y_true and y_pred are numpy arrays of the same length.\n",
    "    return ((y_true - y_pred)**2).mean()\n",
    "\n",
    "\n",
    "class OurNeuralNetwork:\n",
    "    '''\n",
    "  A neural network with:\n",
    "    - 2 inputs\n",
    "    - a hidden layer with 2 neurons (h1, h2)\n",
    "    - an output layer with 1 neuron (o1)\n",
    "\n",
    "  *** DISCLAIMER ***:\n",
    "  The code below is intended to be simple and educational, NOT optimal.\n",
    "  Real neural net code looks nothing like this. DO NOT use this code.\n",
    "  Instead, read/run it to understand how this specific network works.\n",
    "  '''\n",
    "\n",
    "    def __init__(self):\n",
    "        # Weights\n",
    "        self.w1 = np.random.normal()\n",
    "        self.w2 = np.random.normal()\n",
    "        self.w3 = np.random.normal()\n",
    "        self.w4 = np.random.normal()\n",
    "        self.w5 = np.random.normal()\n",
    "        self.w6 = np.random.normal()\n",
    "\n",
    "        # Biases\n",
    "        self.b1 = np.random.normal()\n",
    "        self.b2 = np.random.normal()\n",
    "        self.b3 = np.random.normal()\n",
    "\n",
    "    def feedforward(self, x):\n",
    "        # x is a numpy array with 2 elements.\n",
    "        h1 = sigmoid(self.w1 * x[0] + self.w2 * x[1] + self.b1)\n",
    "        h2 = sigmoid(self.w3 * x[0] + self.w4 * x[1] + self.b2)\n",
    "        o1 = sigmoid(self.w5 * h1 + self.w6 * h2 + self.b3)\n",
    "        return o1\n",
    "\n",
    "    def train(self, data, all_y_trues):\n",
    "        '''\n",
    "    - data is a (n x 2) numpy array, n = # of samples in the dataset.\n",
    "    - all_y_trues is a numpy array with n elements.\n",
    "      Elements in all_y_trues correspond to those in data.\n",
    "    '''\n",
    "        learn_rate = 0.1\n",
    "        epochs = 1000  # number of times to loop through the entire dataset\n",
    "        res = []\n",
    "        for epoch in range(epochs):\n",
    "            for x, y_true in zip(data, all_y_trues):\n",
    "                # --- Do a feedforward (we'll need these values later)\n",
    "                sum_h1 = self.w1 * x[0] + self.w2 * x[1] + self.b1\n",
    "                h1 = sigmoid(sum_h1)\n",
    "\n",
    "                sum_h2 = self.w3 * x[0] + self.w4 * x[1] + self.b2\n",
    "                h2 = sigmoid(sum_h2)\n",
    "\n",
    "                sum_o1 = self.w5 * h1 + self.w6 * h2 + self.b3\n",
    "                o1 = sigmoid(sum_o1)\n",
    "                y_pred = o1\n",
    "\n",
    "                # --- Calculate partial derivatives.\n",
    "                # --- Naming: d_L_d_w1 represents \"partial L / partial w1\"\n",
    "                d_L_d_ypred = -2 * (y_true - y_pred)\n",
    "\n",
    "                # Neuron o1\n",
    "                d_ypred_d_w5 = h1 * deriv_sigmoid(sum_o1)\n",
    "                d_ypred_d_w6 = h2 * deriv_sigmoid(sum_o1)\n",
    "                d_ypred_d_b3 = deriv_sigmoid(sum_o1)\n",
    "\n",
    "                d_ypred_d_h1 = self.w5 * deriv_sigmoid(sum_o1)\n",
    "                d_ypred_d_h2 = self.w6 * deriv_sigmoid(sum_o1)\n",
    "\n",
    "                # Neuron h1\n",
    "                d_h1_d_w1 = x[0] * deriv_sigmoid(sum_h1)\n",
    "                d_h1_d_w2 = x[1] * deriv_sigmoid(sum_h1)\n",
    "                d_h1_d_b1 = deriv_sigmoid(sum_h1)\n",
    "\n",
    "                # Neuron h2\n",
    "                d_h2_d_w3 = x[0] * deriv_sigmoid(sum_h2)\n",
    "                d_h2_d_w4 = x[1] * deriv_sigmoid(sum_h2)\n",
    "                d_h2_d_b2 = deriv_sigmoid(sum_h2)\n",
    "\n",
    "                # --- Update weights and biases\n",
    "                # Neuron h1\n",
    "                self.w1 -= learn_rate * d_L_d_ypred * d_ypred_d_h1 * d_h1_d_w1\n",
    "                self.w2 -= learn_rate * d_L_d_ypred * d_ypred_d_h1 * d_h1_d_w2\n",
    "                self.b1 -= learn_rate * d_L_d_ypred * d_ypred_d_h1 * d_h1_d_b1\n",
    "\n",
    "                # Neuron h2\n",
    "                self.w3 -= learn_rate * d_L_d_ypred * d_ypred_d_h2 * d_h2_d_w3\n",
    "                self.w4 -= learn_rate * d_L_d_ypred * d_ypred_d_h2 * d_h2_d_w4\n",
    "                self.b2 -= learn_rate * d_L_d_ypred * d_ypred_d_h2 * d_h2_d_b2\n",
    "\n",
    "                # Neuron o1\n",
    "                self.w5 -= learn_rate * d_L_d_ypred * d_ypred_d_w5\n",
    "                self.w6 -= learn_rate * d_L_d_ypred * d_ypred_d_w6\n",
    "                self.b3 -= learn_rate * d_L_d_ypred * d_ypred_d_b3\n",
    "\n",
    "            # --- Calculate total loss at the end of each epoch\n",
    "            if epoch % 10 == 0:\n",
    "                y_preds = np.apply_along_axis(self.feedforward, 1, data)\n",
    "                loss = mse_loss(all_y_trues, y_preds)\n",
    "                #print(\"Epoch %d loss: %.3f\" % (epoch, loss))\n",
    "                res += [epoch, loss]\n",
    "        return res\n",
    "\n",
    "\n",
    "# Define dataset\n",
    "data = np.array([\n",
    "    [-2, -1],  # Alice\n",
    "    [25, 6],  # Bob\n",
    "    [17, 4],  # Charlie\n",
    "    [-15, -6],  # Diana\n",
    "])\n",
    "all_y_trues = np.array([\n",
    "    1,  # Alice\n",
    "    0,  # Bob\n",
    "    0,  # Charlie\n",
    "    1,  # Diana\n",
    "])\n",
    "\n",
    "# Train our neural network!\n",
    "network = OurNeuralNetwork()\n",
    "res = network.train(data, all_y_trues)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-14T14:29:20.608657Z",
     "start_time": "2019-12-14T14:29:20.386876Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(np.array(res[0::2]), np.array(res[1::2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## produce variable in large quantity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-13T13:33:14.991387Z",
     "start_time": "2019-12-13T13:33:14.983509Z"
    }
   },
   "outputs": [],
   "source": [
    "\"a{}\".format(1) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Robust code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-14T06:27:32.047900Z",
     "start_time": "2019-12-14T06:27:32.017249Z"
    }
   },
   "outputs": [],
   "source": [
    "def add(x: int, y: int):\n",
    "    if type(x) == int and type(y) == int:\n",
    "        return x + y\n",
    "    else:\n",
    "        print(\"Argument should has data type as int\")\n",
    "        raise (TypeError)\n",
    "\n",
    "\n",
    "add(2, 3)\n",
    "add(2, 3.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-14T06:44:07.788388Z",
     "start_time": "2019-12-14T06:44:07.763719Z"
    }
   },
   "outputs": [],
   "source": [
    "x = [1,2.0,3]\n",
    "\n",
    "any(i > 2 for i in x)\n",
    "any([i > 2 for i in x])\n",
    "all(i > 2 for i in x)\n",
    "all(i == int for i in x)\n",
    "all(i == type for i in x)\n",
    "all(type(i) == int for i in x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-14T06:42:29.780144Z",
     "start_time": "2019-12-14T06:42:29.746299Z"
    }
   },
   "outputs": [],
   "source": [
    "def add(*args):\n",
    "    if all(type(i) == int for i in args):\n",
    "        return sum(args)\n",
    "    else:\n",
    "        print(\"Argument should has data type as int\")\n",
    "        raise (TypeError)\n",
    "\n",
    "add(2, 3)\n",
    "add(2, 3.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A little modification of original approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-14T12:32:47.583599Z",
     "start_time": "2019-12-14T12:32:47.544914Z"
    }
   },
   "outputs": [],
   "source": [
    "def mse_loss(y_true, y_pred):\n",
    "    if type(y_true) == np.ndarray and type(y_pred) == np.ndarray:\n",
    "        return ((y_true - y_pred)**2).mean()\n",
    "    else:\n",
    "        raise (TypeError)\n",
    "        \n",
    "mse_loss(np.array([1, 2]), np.array([3, 4]))\n",
    "mse_loss(np.array([1, 2]), [3, 4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-14T13:51:29.521129Z",
     "start_time": "2019-12-14T13:51:29.062165Z"
    }
   },
   "outputs": [],
   "source": [
    "dot = lambda w, x: w[0] * x[0] + w[1] * x[1]\n",
    "sigmoid = lambda x: 1 / (1 + np.exp(-x))\n",
    "mse_loss = lambda y_true, y_pred: ((y_true - y_pred)**2).mean()\n",
    "deriv_sigmoid = lambda x: sigmoid(x) * (1 - sigmoid(x))\n",
    "\n",
    "\n",
    "class OurNeuralNetwork:\n",
    "    def __init__(self):\n",
    "        number_of_weights = 6\n",
    "        number_of_bias = 3\n",
    "\n",
    "        self.w = [0] * number_of_weights\n",
    "        for i in range(1, number_of_weights):\n",
    "            self.w[i] = np.random.normal()\n",
    "\n",
    "        self.b = [0] * number_of_bias\n",
    "        for i in range(1, number_of_bias):\n",
    "            self.b[i] = np.random.normal()\n",
    "\n",
    "    def feedforward(self, x):\n",
    "        h = [0] * 2\n",
    "        h[0] = sigmoid(dot(self.w[0:2], x) + self.b[0])\n",
    "        h[1] = sigmoid(dot(self.w[2:4], x) + self.b[1])\n",
    "        o1 = sigmoid(dot(self.w[4:6], h) + self.b[2])\n",
    "        return o1\n",
    "\n",
    "    def train(self, data, all_y_trues):\n",
    "        learn_rate = 0.1\n",
    "        epochs = 1000\n",
    "        res = []\n",
    "        for epoch in range(epochs):\n",
    "            for x, y_true in zip(data, all_y_trues):\n",
    "\n",
    "                h = [0] * 2\n",
    "                \"\"\"\n",
    "                s = [0] * 3\n",
    "                \n",
    "                for i in range(3):\n",
    "                    s[i] = dot(self.w[2i: 2(i+1)], self.b[i])\n",
    "                    h[i] = sigmoid(s[i])\n",
    "                \"\"\"\n",
    "                sum_h0 = dot(self.w[0:2], x) + self.b[0]\n",
    "                h[0] = sigmoid(sum_h0)\n",
    "                sum_h1 = dot(self.w[2:4], x) + self.b[1]\n",
    "                h[1] = sigmoid(sum_h1)\n",
    "\n",
    "                sum_o1 = dot(self.w[4:6], h) + self.b[2]\n",
    "                y_pred = sigmoid(sum_o1)\n",
    "\n",
    "                d_L_d_ypred = -2 * (y_true - y_pred)\n",
    "\n",
    "                # Neuron o1\n",
    "                d_ypred_d_w4 = h[0] * deriv_sigmoid(sum_o1)\n",
    "                d_ypred_d_w5 = h[1] * deriv_sigmoid(sum_o1)\n",
    "                d_ypred_d_b2 = deriv_sigmoid(sum_o1)\n",
    "\n",
    "                d_ypred_d_h0 = self.w[4] * deriv_sigmoid(sum_o1)\n",
    "                d_ypred_d_h1 = self.w[5] * deriv_sigmoid(sum_o1)\n",
    "\n",
    "                # Neuron h1\n",
    "                d_h0_d_w0 = x[0] * deriv_sigmoid(sum_h0)\n",
    "                d_h0_d_w1 = x[1] * deriv_sigmoid(sum_h0)\n",
    "                d_h0_d_b0 = deriv_sigmoid(sum_h0)\n",
    "\n",
    "                # Neuron h2\n",
    "                d_h1_d_w2 = x[0] * deriv_sigmoid(sum_h1)\n",
    "                d_h1_d_w3 = x[1] * deriv_sigmoid(sum_h1)\n",
    "                d_h1_d_b1 = deriv_sigmoid(sum_h1)\n",
    "\n",
    "                # --- Update weights and biases\n",
    "                # Neuron h1\n",
    "                self.w[0] -= learn_rate * d_L_d_ypred * d_ypred_d_h0 * d_h0_d_w0\n",
    "                self.w[1] -= learn_rate * d_L_d_ypred * d_ypred_d_h0 * d_h0_d_w1\n",
    "                self.b[0] -= learn_rate * d_L_d_ypred * d_ypred_d_h0 * d_h0_d_b0\n",
    "\n",
    "                # Neuron h2\n",
    "                self.w[2] -= learn_rate * d_L_d_ypred * d_ypred_d_h1 * d_h1_d_w2\n",
    "                self.w[3] -= learn_rate * d_L_d_ypred * d_ypred_d_h1 * d_h1_d_w3\n",
    "                self.b[1] -= learn_rate * d_L_d_ypred * d_ypred_d_h1 * d_h1_d_b1\n",
    "\n",
    "                # Neuron o1\n",
    "                self.w[4] -= learn_rate * d_L_d_ypred * d_ypred_d_w4\n",
    "                self.w[5] -= learn_rate * d_L_d_ypred * d_ypred_d_w5\n",
    "                self.b[2] -= learn_rate * d_L_d_ypred * d_ypred_d_b2\n",
    "\n",
    "            # --- Calculate total loss at the end of each epoch\n",
    "            if epoch % 10 == 0:\n",
    "                y_preds = np.apply_along_axis(self.feedforward, 1, data)\n",
    "                loss = mse_loss(all_y_trues, y_preds)\n",
    "                #print(\"Epoch %d loss: %.3f\" % (epoch, loss))\n",
    "                res += [epoch, loss]\n",
    "        return res\n",
    "\n",
    "\n",
    "# Define dataset\n",
    "data = np.array([\n",
    "    [-2, -1],  # Alice\n",
    "    [25, 6],  # Bob\n",
    "    [17, 4],  # Charlie\n",
    "    [-15, -6],  # Diana\n",
    "])\n",
    "all_y_trues = np.array([\n",
    "    1,  # Alice\n",
    "    0,  # Bob\n",
    "    0,  # Charlie\n",
    "    1,  # Diana\n",
    "])\n",
    "\n",
    "# Train our neural network!\n",
    "network = OurNeuralNetwork()\n",
    "res = network.train(data, all_y_trues)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-14T13:51:30.956177Z",
     "start_time": "2019-12-14T13:51:30.747974Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(np.array(res[0::2]), np.array(res[1::2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Abstract the pattern"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-14T12:30:08.891128Z",
     "start_time": "2019-12-14T12:30:08.886153Z"
    }
   },
   "source": [
    "<span style=\"font-family:New York Times; font-size:1.2em; color:red;\">\n",
    "\n",
    "\n",
    "ToDo: Apply the principle Explicit is better than implicit thoroughly\n",
    "\n",
    "* or hard to locate where went wrong\n",
    "* It takes a long time compare to the original approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-14T14:01:43.052370Z",
     "start_time": "2019-12-14T14:01:40.383188Z"
    }
   },
   "outputs": [],
   "source": [
    "import sympy\n",
    "from sympy import Lambda\n",
    "from  sympy import abc, Derivative, diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-14T14:40:23.390667Z",
     "start_time": "2019-12-14T14:40:23.383333Z"
    }
   },
   "outputs": [],
   "source": [
    "dot = lambda w, x, bias: w[0] * x[0] + w[1] * x[1] + bias\n",
    "sigmoid = Lambda(abc.x, 1 / (1 + sympy.exp(-abc.x)))\n",
    "dsig = Lambda(abc.x, diff(sigmoid(abc.x), abc.x))\n",
    "mse_loss = lambda y_true, y_pred: ((y_true - y_pred)**2).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-14T14:40:25.060636Z",
     "start_time": "2019-12-14T14:40:24.971973Z"
    }
   },
   "outputs": [],
   "source": [
    "class OurNeuralNetwork:\n",
    "    def __init__(self):\n",
    "        number_of_weights = 6\n",
    "        number_of_bias = 3\n",
    "\n",
    "        self.w = [0] * number_of_weights\n",
    "        for i in range(1, number_of_weights):\n",
    "            self.w[i] = np.random.normal()\n",
    "\n",
    "        self.b = [0] * number_of_bias\n",
    "        for i in range(0, number_of_bias):\n",
    "            self.b[i] = np.random.normal()\n",
    "\n",
    "    def feedforward(self, x):\n",
    "        h = [0] * 2\n",
    "        h[0] = sigmoid(dot(self.w[0:2], x, self.b[0]))\n",
    "        h[1] = sigmoid(dot(self.w[2:4], x, self.b[1]))\n",
    "        o1 = sigmoid(dot(self.w[4:6], h, self.b[2]))\n",
    "        return o1\n",
    "\n",
    "    def train(self, data, all_y_trues):\n",
    "        learn_rate = 0.1\n",
    "        epochs = 1000\n",
    "        res = []\n",
    "        for epoch in range(epochs):\n",
    "            for x, y_true in zip(data, all_y_trues):\n",
    "\n",
    "                s = [0] * 3\n",
    "                h = [0] * 2\n",
    "                for i in range(2):\n",
    "                    s[i] = dot(self.w[2*i : 2*(i + 1)], x, self.b[i])\n",
    "                    h[i] = sigmoid(s[i])\n",
    "\n",
    "                s[2] = dot(self.w[4:6], h, self.b[2])\n",
    "                #y_pred = s[2]\n",
    "                y_pred = sigmoid(s[2])\n",
    "\n",
    "                d_L_d_ypred = -2 * (y_true - y_pred)\n",
    "\n",
    "                ww = sympy.Array([abc.a, abc.b])\n",
    "                xx = sympy.Array([abc.m, abc.n])\n",
    "                bias = abc.kappa\n",
    "                table1 = sympy.derive_by_array(dot(ww, xx, bias), \\\n",
    "                                               [ww[0], ww[1], xx[0], xx[1], bias])\n",
    "\n",
    "                d_ypred = [0] * 5\n",
    "                for i in range(5):\n",
    "                    d_ypred[i] = table1[i].subs({\n",
    "                        ww[0]: self.w[4],\n",
    "                        ww[1]: self.w[5],\n",
    "                        xx[0]: h[0],\n",
    "                        xx[1]: h[1],\n",
    "                        bias: self.b[2]\n",
    "                    }) * dsig(abc.lamda).subs({abc.lamda: s[2]})\n",
    "\n",
    "                table2 = sympy.derive_by_array(dot(ww, xx, bias),\n",
    "                                               [ww[0], ww[1], bias])\n",
    "                d_h0 = [0] * 3\n",
    "                d_h1 = [0] * 3\n",
    "                for i in range(3):\n",
    "                    d_h0[i] = table2[i].subs({xx[0]: x[0], xx[1]: x[1],bias: self.b[0]}) *\\\n",
    "                              dsig(abc.lamda).subs({abc.lamda:s[0]})\n",
    "\n",
    "                    d_h1[i] = table2[i].subs({\n",
    "                        xx[0]: x[0],\n",
    "                        xx[1]: x[1],\n",
    "                        bias: self.b[1]\n",
    "                    }) * dsig(abc.lamda).subs({abc.lamda: s[1]})\n",
    "                \n",
    "                # Neuron h1\n",
    "                self.w[0] -= learn_rate * d_L_d_ypred * d_ypred[2] * d_h0[0]\n",
    "                self.w[1] -= learn_rate * d_L_d_ypred * d_ypred[2] * d_h0[1]\n",
    "                self.b[0] -= learn_rate * d_L_d_ypred * d_ypred[2] * d_h0[2]\n",
    "                \n",
    "                # Neuron h2\n",
    "                self.w[2] -= learn_rate * d_L_d_ypred * d_ypred[3] * d_h1[0]\n",
    "                self.w[3] -= learn_rate * d_L_d_ypred * d_ypred[3] * d_h1[1]\n",
    "                self.b[1] -= learn_rate * d_L_d_ypred * d_ypred[3] * d_h1[2]\n",
    "\n",
    "                # Neuron o1\n",
    "                self.w[4] -= learn_rate * d_L_d_ypred * d_ypred[0]\n",
    "                self.w[5] -= learn_rate * d_L_d_ypred * d_ypred[1]\n",
    "                self.b[2] -= learn_rate * d_L_d_ypred * d_ypred[4]\n",
    "\n",
    "            if epoch % 10 == 0:\n",
    "                y_preds = np.apply_along_axis(self.feedforward, 1, data)\n",
    "                loss = mse_loss(all_y_trues, y_preds)\n",
    "                res += [epoch, loss]\n",
    "        return res\n",
    "\n",
    "\n",
    "# Define dataset\n",
    "data = np.array([\n",
    "    [-2, -1],  # Alice\n",
    "    [25, 6],  # Bob\n",
    "    [17, 4],  # Charlie\n",
    "    [-15, -6],  # Diana\n",
    "])\n",
    "all_y_trues = np.array([\n",
    "    1,  # Alice\n",
    "    0,  # Bob\n",
    "    0,  # Charlie\n",
    "    1,  # Diana\n",
    "])\n",
    "\n",
    "# Train our neural network!\n",
    "network = OurNeuralNetwork()\n",
    "res = network.train(data, all_y_trues)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-14T14:38:51.084983Z",
     "start_time": "2019-12-14T14:38:50.688851Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(np.array(res[0::2]), np.array(res[1::2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-14T12:15:20.732657Z",
     "start_time": "2019-12-14T12:15:20.695993Z"
    }
   },
   "source": [
    "## new version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-14T14:35:07.450255Z",
     "start_time": "2019-12-14T14:35:07.434323Z"
    }
   },
   "outputs": [],
   "source": [
    "dot = lambda w, x, bias: w[0] * x[0] + w[1] * x[1] + bias\n",
    "sigmoid = lambda x: 1 / (1 + np.exp(-x))\n",
    "dsig = lambda x: sigmoid(x) * (1- sigmoid(x))\n",
    "mse_loss = lambda y_true, y_pred: ((y_true - y_pred)**2).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-14T14:35:08.143288Z",
     "start_time": "2019-12-14T14:35:07.674188Z"
    }
   },
   "outputs": [],
   "source": [
    "class OurNeuralNetwork:\n",
    "    def __init__(self):\n",
    "        number_of_weights = 6\n",
    "        number_of_bias = 3\n",
    "\n",
    "        self.w = [0] * number_of_weights\n",
    "        for i in range(1, number_of_weights):\n",
    "            self.w[i] = np.random.normal()\n",
    "\n",
    "        self.b = [0] * number_of_bias\n",
    "        for i in range(0, number_of_bias):\n",
    "            self.b[i] = np.random.normal()\n",
    "\n",
    "    def feedforward(self, x):\n",
    "        h = [0] * 2\n",
    "        h[0] = sigmoid(dot(self.w[0:2], x, self.b[0]))\n",
    "        h[1] = sigmoid(dot(self.w[2:4], x, self.b[1]))\n",
    "        o1 = sigmoid(dot(self.w[4:6], h, self.b[2]))\n",
    "        return o1\n",
    "\n",
    "    def train(self, data, all_y_trues):\n",
    "        learn_rate = 0.1\n",
    "        epochs = 1000\n",
    "        res = []\n",
    "        for epoch in range(epochs):\n",
    "            for x, y_true in zip(data, all_y_trues):\n",
    "\n",
    "                s = [0] * 3\n",
    "                h = [0] * 2\n",
    "                for i in range(2):\n",
    "                    s[i] = dot(self.w[2*i : 2*(i + 1)], x, self.b[i])\n",
    "                    h[i] = sigmoid(s[i])\n",
    "\n",
    "                s[2] = dot(self.w[4:6], h, self.b[2])\n",
    "                y_pred = sigmoid(s[2])\n",
    "\n",
    "                d_L_d_ypred = -2 * (y_true - y_pred)\n",
    "                \n",
    "                \"\"\"\n",
    "                ww = sympy.Array([abc.a, abc.b])\n",
    "                xx = sympy.Array([abc.m, abc.n])\n",
    "                bias = abc.kappa\n",
    "                \n",
    "                table = sympy.derive_by_array(dot(ww, xx, bias), [ww[0], ww[1], xx[0], xx[1], bias])\n",
    "               \n",
    "                table2 = table.subs({\n",
    "                        ww[0]: self.w[4],\n",
    "                        ww[1]: self.w[5],\n",
    "                        xx[0]: h[0],\n",
    "                        xx[1]: h[1],\n",
    "                        bias: self.b[2]\n",
    "                    })\n",
    "                \"\"\"\n",
    "                table2 = [h[0], h[1], self.w[4], self.w[5], 1]\n",
    "                \n",
    "                \n",
    "                d_ypred = [0] * 5\n",
    "                for i in range(5):\n",
    "                    d_ypred[i] = table2[i] * dsig(s[2])\n",
    "                \"\"\"\n",
    "                table = sympy.derive_by_array(dot(ww, xx, bias), [ww[0], ww[1], bias])\n",
    "                table1 = table.subs({xx[0]: x[0], xx[1]: x[1], bias: self.b[1]}) \n",
    "                table0 = table.subs({xx[0]: x[0], xx[1]: x[1], bias: self.b[0]}) \n",
    "                \"\"\"\n",
    "                table1 = [self.w[2], self.w[3], 1]\n",
    "                table0 = [self.w[0], self.w[1], 1]\n",
    "                d_h0 = [0] * 3\n",
    "                d_h1 = [0] * 3\n",
    "                for i in range(3):\n",
    "                    d_h0[i] = table0[i] * dsig(s[0])\n",
    "                    d_h1[i] = table1[i] * dsig(s[1])\n",
    "                \n",
    "                # Neuron h1\n",
    "                self.w[0] -= learn_rate * d_L_d_ypred * d_ypred[2] * d_h0[0]\n",
    "                self.w[1] -= learn_rate * d_L_d_ypred * d_ypred[2] * d_h0[1]\n",
    "                self.b[0] -= learn_rate * d_L_d_ypred * d_ypred[2] * d_h0[2]\n",
    "                \n",
    "                # Neuron h2\n",
    "                self.w[2] -= learn_rate * d_L_d_ypred * d_ypred[3] * d_h1[0]\n",
    "                self.w[3] -= learn_rate * d_L_d_ypred * d_ypred[3] * d_h1[1]\n",
    "                self.b[1] -= learn_rate * d_L_d_ypred * d_ypred[3] * d_h1[2]\n",
    "\n",
    "                # Neuron o1\n",
    "                self.w[4] -= learn_rate * d_L_d_ypred * d_ypred[0]\n",
    "                self.w[5] -= learn_rate * d_L_d_ypred * d_ypred[1]\n",
    "                self.b[2] -= learn_rate * d_L_d_ypred * d_ypred[4]\n",
    "\n",
    "            if epoch % 10 == 0:\n",
    "                y_preds = np.apply_along_axis(self.feedforward, 1, data)\n",
    "                loss = mse_loss(all_y_trues, y_preds)\n",
    "                res += [epoch, loss]\n",
    "        return res\n",
    "\n",
    "\n",
    "# Define dataset\n",
    "data = np.array([\n",
    "    [-2, -1],  # Alice\n",
    "    [25, 6],  # Bob\n",
    "    [17, 4],  # Charlie\n",
    "    [-15, -6],  # Diana\n",
    "])\n",
    "all_y_trues = np.array([\n",
    "    1,  # Alice\n",
    "    0,  # Bob\n",
    "    0,  # Charlie\n",
    "    1,  # Diana\n",
    "])\n",
    "\n",
    "# Train our neural network!\n",
    "network = OurNeuralNetwork()\n",
    "res = network.train(data, all_y_trues)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-14T14:35:08.352763Z",
     "start_time": "2019-12-14T14:35:08.145976Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(np.array(res[0::2]), np.array(res[1::2]))"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
