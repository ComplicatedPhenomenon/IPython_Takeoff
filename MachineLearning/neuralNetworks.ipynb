{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-19T08:49:45.980620Z",
     "start_time": "2019-12-19T08:49:45.976690Z"
    }
   },
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-19T08:49:48.808999Z",
     "start_time": "2019-12-19T08:49:46.272159Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-19T08:49:50.661942Z",
     "start_time": "2019-12-19T08:49:48.814127Z"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-19T08:49:50.671165Z",
     "start_time": "2019-12-19T08:49:50.664817Z"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-13T13:13:00.931433Z",
     "start_time": "2019-12-13T13:13:00.828186Z"
    }
   },
   "source": [
    "https://victorzhou.com/blog/intro-to-neural-networks/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# original approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://victorzhou.com/27cf280166d7159c0465a58c68f99b39/network3.svg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-14T14:29:20.383785Z",
     "start_time": "2019-12-14T14:29:19.993110Z"
    }
   },
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    # Sigmoid activation function: f(x) = 1 / (1 + e^(-x))\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "\n",
    "def deriv_sigmoid(x):\n",
    "    # Derivative of sigmoid: f'(x) = f(x) * (1 - f(x))\n",
    "    fx = sigmoid(x)\n",
    "    return fx * (1 - fx)\n",
    "\n",
    "\n",
    "def mse_loss(y_true, y_pred):\n",
    "    # y_true and y_pred are numpy arrays of the same length.\n",
    "    return ((y_true - y_pred)**2).mean()\n",
    "\n",
    "\n",
    "class OurNeuralNetwork:\n",
    "    '''\n",
    "  A neural network with:\n",
    "    - 2 inputs\n",
    "    - a hidden layer with 2 neurons (h1, h2)\n",
    "    - an output layer with 1 neuron (o1)\n",
    "\n",
    "  *** DISCLAIMER ***:\n",
    "  The code below is intended to be simple and educational, NOT optimal.\n",
    "  Real neural net code looks nothing like this. DO NOT use this code.\n",
    "  Instead, read/run it to understand how this specific network works.\n",
    "  '''\n",
    "\n",
    "    def __init__(self):\n",
    "        # Weights\n",
    "        self.w1 = np.random.normal()\n",
    "        self.w2 = np.random.normal()\n",
    "        self.w3 = np.random.normal()\n",
    "        self.w4 = np.random.normal()\n",
    "        self.w5 = np.random.normal()\n",
    "        self.w6 = np.random.normal()\n",
    "\n",
    "        # Biases\n",
    "        self.b1 = np.random.normal()\n",
    "        self.b2 = np.random.normal()\n",
    "        self.b3 = np.random.normal()\n",
    "\n",
    "    def feedforward(self, x):\n",
    "        # x is a numpy array with 2 elements.\n",
    "        h1 = sigmoid(self.w1 * x[0] + self.w2 * x[1] + self.b1)\n",
    "        h2 = sigmoid(self.w3 * x[0] + self.w4 * x[1] + self.b2)\n",
    "        o1 = sigmoid(self.w5 * h1 + self.w6 * h2 + self.b3)\n",
    "        return o1\n",
    "\n",
    "    def train(self, data, all_y_trues):\n",
    "        '''\n",
    "    - data is a (n x 2) numpy array, n = # of samples in the dataset.\n",
    "    - all_y_trues is a numpy array with n elements.\n",
    "      Elements in all_y_trues correspond to those in data.\n",
    "    '''\n",
    "        learn_rate = 0.1\n",
    "        epochs = 1000  # number of times to loop through the entire dataset\n",
    "        res = []\n",
    "        for epoch in range(epochs):\n",
    "            for x, y_true in zip(data, all_y_trues):\n",
    "                # --- Do a feedforward (we'll need these values later)\n",
    "                sum_h1 = self.w1 * x[0] + self.w2 * x[1] + self.b1\n",
    "                h1 = sigmoid(sum_h1)\n",
    "\n",
    "                sum_h2 = self.w3 * x[0] + self.w4 * x[1] + self.b2\n",
    "                h2 = sigmoid(sum_h2)\n",
    "\n",
    "                sum_o1 = self.w5 * h1 + self.w6 * h2 + self.b3\n",
    "                o1 = sigmoid(sum_o1)\n",
    "                y_pred = o1\n",
    "\n",
    "                # --- Calculate partial derivatives.\n",
    "                # --- Naming: d_L_d_w1 represents \"partial L / partial w1\"\n",
    "                d_L_d_ypred = -2 * (y_true - y_pred)\n",
    "\n",
    "                # Neuron o1\n",
    "                d_ypred_d_w5 = h1 * deriv_sigmoid(sum_o1)\n",
    "                d_ypred_d_w6 = h2 * deriv_sigmoid(sum_o1)\n",
    "                d_ypred_d_b3 = deriv_sigmoid(sum_o1)\n",
    "\n",
    "                d_ypred_d_h1 = self.w5 * deriv_sigmoid(sum_o1)\n",
    "                d_ypred_d_h2 = self.w6 * deriv_sigmoid(sum_o1)\n",
    "\n",
    "                # Neuron h1\n",
    "                d_h1_d_w1 = x[0] * deriv_sigmoid(sum_h1)\n",
    "                d_h1_d_w2 = x[1] * deriv_sigmoid(sum_h1)\n",
    "                d_h1_d_b1 = deriv_sigmoid(sum_h1)\n",
    "\n",
    "                # Neuron h2\n",
    "                d_h2_d_w3 = x[0] * deriv_sigmoid(sum_h2)\n",
    "                d_h2_d_w4 = x[1] * deriv_sigmoid(sum_h2)\n",
    "                d_h2_d_b2 = deriv_sigmoid(sum_h2)\n",
    "\n",
    "                # --- Update weights and biases\n",
    "                # Neuron h1\n",
    "                self.w1 -= learn_rate * d_L_d_ypred * d_ypred_d_h1 * d_h1_d_w1\n",
    "                self.w2 -= learn_rate * d_L_d_ypred * d_ypred_d_h1 * d_h1_d_w2\n",
    "                self.b1 -= learn_rate * d_L_d_ypred * d_ypred_d_h1 * d_h1_d_b1\n",
    "\n",
    "                # Neuron h2\n",
    "                self.w3 -= learn_rate * d_L_d_ypred * d_ypred_d_h2 * d_h2_d_w3\n",
    "                self.w4 -= learn_rate * d_L_d_ypred * d_ypred_d_h2 * d_h2_d_w4\n",
    "                self.b2 -= learn_rate * d_L_d_ypred * d_ypred_d_h2 * d_h2_d_b2\n",
    "\n",
    "                # Neuron o1\n",
    "                self.w5 -= learn_rate * d_L_d_ypred * d_ypred_d_w5\n",
    "                self.w6 -= learn_rate * d_L_d_ypred * d_ypred_d_w6\n",
    "                self.b3 -= learn_rate * d_L_d_ypred * d_ypred_d_b3\n",
    "\n",
    "            # --- Calculate total loss at the end of each epoch\n",
    "            if epoch % 10 == 0:\n",
    "                y_preds = np.apply_along_axis(self.feedforward, 1, data)\n",
    "                loss = mse_loss(all_y_trues, y_preds)\n",
    "                #print(\"Epoch %d loss: %.3f\" % (epoch, loss))\n",
    "                res += [epoch, loss]\n",
    "        return res\n",
    "\n",
    "\n",
    "# Define dataset\n",
    "data = np.array([\n",
    "    [-2, -1],  # Alice\n",
    "    [25, 6],  # Bob\n",
    "    [17, 4],  # Charlie\n",
    "    [-15, -6],  # Diana\n",
    "])\n",
    "all_y_trues = np.array([\n",
    "    1,  # Alice\n",
    "    0,  # Bob\n",
    "    0,  # Charlie\n",
    "    1,  # Diana\n",
    "])\n",
    "\n",
    "# Train our neural network!\n",
    "network = OurNeuralNetwork()\n",
    "res = network.train(data, all_y_trues)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-14T14:29:20.608657Z",
     "start_time": "2019-12-14T14:29:20.386876Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(np.array(res[0::2]), np.array(res[1::2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## produce variable in large quantity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-13T13:33:14.991387Z",
     "start_time": "2019-12-13T13:33:14.983509Z"
    }
   },
   "outputs": [],
   "source": [
    "\"a{}\".format(1) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Robust code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-14T06:27:32.047900Z",
     "start_time": "2019-12-14T06:27:32.017249Z"
    }
   },
   "outputs": [],
   "source": [
    "def add(x: int, y: int):\n",
    "    if type(x) == int and type(y) == int:\n",
    "        return x + y\n",
    "    else:\n",
    "        print(\"Argument should has data type as int\")\n",
    "        raise (TypeError)\n",
    "\n",
    "\n",
    "add(2, 3)\n",
    "add(2, 3.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-14T06:44:07.788388Z",
     "start_time": "2019-12-14T06:44:07.763719Z"
    }
   },
   "outputs": [],
   "source": [
    "x = [1,2.0,3]\n",
    "\n",
    "any(i > 2 for i in x)\n",
    "any([i > 2 for i in x])\n",
    "all(i > 2 for i in x)\n",
    "all(i == int for i in x)\n",
    "all(i == type for i in x)\n",
    "all(type(i) == int for i in x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-14T06:42:29.780144Z",
     "start_time": "2019-12-14T06:42:29.746299Z"
    }
   },
   "outputs": [],
   "source": [
    "def add(*args):\n",
    "    if all(type(i) == int for i in args):\n",
    "        return sum(args)\n",
    "    else:\n",
    "        print(\"Argument should has data type as int\")\n",
    "        raise (TypeError)\n",
    "\n",
    "add(2, 3)\n",
    "add(2, 3.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A little modification of original approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-14T12:32:47.583599Z",
     "start_time": "2019-12-14T12:32:47.544914Z"
    }
   },
   "outputs": [],
   "source": [
    "def mse_loss(y_true, y_pred):\n",
    "    if type(y_true) == np.ndarray and type(y_pred) == np.ndarray:\n",
    "        return ((y_true - y_pred)**2).mean()\n",
    "    else:\n",
    "        raise (TypeError)\n",
    "        \n",
    "mse_loss(np.array([1, 2]), np.array([3, 4]))\n",
    "mse_loss(np.array([1, 2]), [3, 4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-14T13:51:29.521129Z",
     "start_time": "2019-12-14T13:51:29.062165Z"
    }
   },
   "outputs": [],
   "source": [
    "dot = lambda w, x: w[0] * x[0] + w[1] * x[1]\n",
    "sigmoid = lambda x: 1 / (1 + np.exp(-x))\n",
    "mse_loss = lambda y_true, y_pred: ((y_true - y_pred)**2).mean()\n",
    "deriv_sigmoid = lambda x: sigmoid(x) * (1 - sigmoid(x))\n",
    "\n",
    "\n",
    "class OurNeuralNetwork:\n",
    "    def __init__(self):\n",
    "        number_of_weights = 6\n",
    "        number_of_bias = 3\n",
    "\n",
    "        self.w = [0] * number_of_weights\n",
    "        for i in range(1, number_of_weights):\n",
    "            self.w[i] = np.random.normal()\n",
    "\n",
    "        self.b = [0] * number_of_bias\n",
    "        for i in range(1, number_of_bias):\n",
    "            self.b[i] = np.random.normal()\n",
    "\n",
    "    def feedforward(self, x):\n",
    "        h = [0] * 2\n",
    "        h[0] = sigmoid(dot(self.w[0:2], x) + self.b[0])\n",
    "        h[1] = sigmoid(dot(self.w[2:4], x) + self.b[1])\n",
    "        o1 = sigmoid(dot(self.w[4:6], h) + self.b[2])\n",
    "        return o1\n",
    "\n",
    "    def train(self, data, all_y_trues):\n",
    "        learn_rate = 0.1\n",
    "        epochs = 1000\n",
    "        res = []\n",
    "        for epoch in range(epochs):\n",
    "            for x, y_true in zip(data, all_y_trues):\n",
    "\n",
    "                h = [0] * 2\n",
    "                \"\"\"\n",
    "                s = [0] * 3\n",
    "                \n",
    "                for i in range(3):\n",
    "                    s[i] = dot(self.w[2i: 2(i+1)], self.b[i])\n",
    "                    h[i] = sigmoid(s[i])\n",
    "                \"\"\"\n",
    "                sum_h0 = dot(self.w[0:2], x) + self.b[0]\n",
    "                h[0] = sigmoid(sum_h0)\n",
    "                sum_h1 = dot(self.w[2:4], x) + self.b[1]\n",
    "                h[1] = sigmoid(sum_h1)\n",
    "\n",
    "                sum_o1 = dot(self.w[4:6], h) + self.b[2]\n",
    "                y_pred = sigmoid(sum_o1)\n",
    "\n",
    "                d_L_d_ypred = -2 * (y_true - y_pred)\n",
    "\n",
    "                # Neuron o1\n",
    "                d_ypred_d_w4 = h[0] * deriv_sigmoid(sum_o1)\n",
    "                d_ypred_d_w5 = h[1] * deriv_sigmoid(sum_o1)\n",
    "                d_ypred_d_b2 = deriv_sigmoid(sum_o1)\n",
    "\n",
    "                d_ypred_d_h0 = self.w[4] * deriv_sigmoid(sum_o1)\n",
    "                d_ypred_d_h1 = self.w[5] * deriv_sigmoid(sum_o1)\n",
    "\n",
    "                # Neuron h1\n",
    "                d_h0_d_w0 = x[0] * deriv_sigmoid(sum_h0)\n",
    "                d_h0_d_w1 = x[1] * deriv_sigmoid(sum_h0)\n",
    "                d_h0_d_b0 = deriv_sigmoid(sum_h0)\n",
    "\n",
    "                # Neuron h2\n",
    "                d_h1_d_w2 = x[0] * deriv_sigmoid(sum_h1)\n",
    "                d_h1_d_w3 = x[1] * deriv_sigmoid(sum_h1)\n",
    "                d_h1_d_b1 = deriv_sigmoid(sum_h1)\n",
    "\n",
    "                # --- Update weights and biases\n",
    "                # Neuron h1\n",
    "                self.w[0] -= learn_rate * d_L_d_ypred * d_ypred_d_h0 * d_h0_d_w0\n",
    "                self.w[1] -= learn_rate * d_L_d_ypred * d_ypred_d_h0 * d_h0_d_w1\n",
    "                self.b[0] -= learn_rate * d_L_d_ypred * d_ypred_d_h0 * d_h0_d_b0\n",
    "\n",
    "                # Neuron h2\n",
    "                self.w[2] -= learn_rate * d_L_d_ypred * d_ypred_d_h1 * d_h1_d_w2\n",
    "                self.w[3] -= learn_rate * d_L_d_ypred * d_ypred_d_h1 * d_h1_d_w3\n",
    "                self.b[1] -= learn_rate * d_L_d_ypred * d_ypred_d_h1 * d_h1_d_b1\n",
    "\n",
    "                # Neuron o1\n",
    "                self.w[4] -= learn_rate * d_L_d_ypred * d_ypred_d_w4\n",
    "                self.w[5] -= learn_rate * d_L_d_ypred * d_ypred_d_w5\n",
    "                self.b[2] -= learn_rate * d_L_d_ypred * d_ypred_d_b2\n",
    "\n",
    "            # --- Calculate total loss at the end of each epoch\n",
    "            if epoch % 10 == 0:\n",
    "                y_preds = np.apply_along_axis(self.feedforward, 1, data)\n",
    "                loss = mse_loss(all_y_trues, y_preds)\n",
    "                #print(\"Epoch %d loss: %.3f\" % (epoch, loss))\n",
    "                res += [epoch, loss]\n",
    "        return res\n",
    "\n",
    "\n",
    "# Define dataset\n",
    "data = np.array([\n",
    "    [-2, -1],  # Alice\n",
    "    [25, 6],  # Bob\n",
    "    [17, 4],  # Charlie\n",
    "    [-15, -6],  # Diana\n",
    "])\n",
    "all_y_trues = np.array([\n",
    "    1,  # Alice\n",
    "    0,  # Bob\n",
    "    0,  # Charlie\n",
    "    1,  # Diana\n",
    "])\n",
    "\n",
    "# Train our neural network!\n",
    "network = OurNeuralNetwork()\n",
    "res = network.train(data, all_y_trues)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-14T13:51:30.956177Z",
     "start_time": "2019-12-14T13:51:30.747974Z"
    }
   },
   "outputs": [],
   "source": [
    "_ = plt.plot(np.array(res[0::2]), np.array(res[1::2]))\n",
    "_ = plt.xlabel(\"eporch\")\n",
    "_ = plt.ylabel(\"loss\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Abstract the pattern"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-14T12:30:08.891128Z",
     "start_time": "2019-12-14T12:30:08.886153Z"
    }
   },
   "source": [
    "<span style=\"font-family:New York Times; font-size:1.2em; color:red;\">\n",
    "\n",
    "\n",
    "ToDo: Apply the principle Explicit is better than implicit thoroughly\n",
    "\n",
    "* or hard to locate where went wrong\n",
    "* It takes a long time compare to the original approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-14T14:01:43.052370Z",
     "start_time": "2019-12-14T14:01:40.383188Z"
    }
   },
   "outputs": [],
   "source": [
    "import sympy\n",
    "from sympy import Lambda\n",
    "from  sympy import abc, Derivative, diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-14T14:40:23.390667Z",
     "start_time": "2019-12-14T14:40:23.383333Z"
    }
   },
   "outputs": [],
   "source": [
    "dot = lambda w, x, bias: w[0] * x[0] + w[1] * x[1] + bias\n",
    "sigmoid = Lambda(abc.x, 1 / (1 + sympy.exp(-abc.x)))\n",
    "dsig = Lambda(abc.x, diff(sigmoid(abc.x), abc.x))\n",
    "mse_loss = lambda y_true, y_pred: ((y_true - y_pred)**2).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-14T14:40:25.060636Z",
     "start_time": "2019-12-14T14:40:24.971973Z"
    }
   },
   "outputs": [],
   "source": [
    "class OurNeuralNetwork:\n",
    "    def __init__(self):\n",
    "        number_of_weights = 6\n",
    "        number_of_bias = 3\n",
    "\n",
    "        self.w = [0] * number_of_weights\n",
    "        for i in range(1, number_of_weights):\n",
    "            self.w[i] = np.random.normal()\n",
    "\n",
    "        self.b = [0] * number_of_bias\n",
    "        for i in range(0, number_of_bias):\n",
    "            self.b[i] = np.random.normal()\n",
    "\n",
    "    def feedforward(self, x):\n",
    "        h = [0] * 2\n",
    "        h[0] = sigmoid(dot(self.w[0:2], x, self.b[0]))\n",
    "        h[1] = sigmoid(dot(self.w[2:4], x, self.b[1]))\n",
    "        o1 = sigmoid(dot(self.w[4:6], h, self.b[2]))\n",
    "        return o1\n",
    "\n",
    "    def train(self, data, all_y_trues):\n",
    "        learn_rate = 0.1\n",
    "        epochs = 1000\n",
    "        res = []\n",
    "        for epoch in range(epochs):\n",
    "            for x, y_true in zip(data, all_y_trues):\n",
    "\n",
    "                s = [0] * 3\n",
    "                h = [0] * 2\n",
    "                for i in range(2):\n",
    "                    s[i] = dot(self.w[2*i : 2*(i + 1)], x, self.b[i])\n",
    "                    h[i] = sigmoid(s[i])\n",
    "\n",
    "                s[2] = dot(self.w[4:6], h, self.b[2])\n",
    "                #y_pred = s[2]\n",
    "                y_pred = sigmoid(s[2])\n",
    "\n",
    "                d_L_d_ypred = -2 * (y_true - y_pred)\n",
    "\n",
    "                ww = sympy.Array([abc.a, abc.b])\n",
    "                xx = sympy.Array([abc.m, abc.n])\n",
    "                bias = abc.kappa\n",
    "                table1 = sympy.derive_by_array(dot(ww, xx, bias), \\\n",
    "                                               [ww[0], ww[1], xx[0], xx[1], bias])\n",
    "\n",
    "                d_ypred = [0] * 5\n",
    "                for i in range(5):\n",
    "                    d_ypred[i] = table1[i].subs({\n",
    "                        ww[0]: self.w[4],\n",
    "                        ww[1]: self.w[5],\n",
    "                        xx[0]: h[0],\n",
    "                        xx[1]: h[1],\n",
    "                        bias: self.b[2]\n",
    "                    }) * dsig(abc.lamda).subs({abc.lamda: s[2]})\n",
    "\n",
    "                table2 = sympy.derive_by_array(dot(ww, xx, bias),\n",
    "                                               [ww[0], ww[1], bias])\n",
    "                d_h0 = [0] * 3\n",
    "                d_h1 = [0] * 3\n",
    "                for i in range(3):\n",
    "                    d_h0[i] = table2[i].subs({xx[0]: x[0], xx[1]: x[1],bias: self.b[0]}) *\\\n",
    "                              dsig(abc.lamda).subs({abc.lamda:s[0]})\n",
    "\n",
    "                    d_h1[i] = table2[i].subs({\n",
    "                        xx[0]: x[0],\n",
    "                        xx[1]: x[1],\n",
    "                        bias: self.b[1]\n",
    "                    }) * dsig(abc.lamda).subs({abc.lamda: s[1]})\n",
    "                \n",
    "                # Neuron h1\n",
    "                self.w[0] -= learn_rate * d_L_d_ypred * d_ypred[2] * d_h0[0]\n",
    "                self.w[1] -= learn_rate * d_L_d_ypred * d_ypred[2] * d_h0[1]\n",
    "                self.b[0] -= learn_rate * d_L_d_ypred * d_ypred[2] * d_h0[2]\n",
    "                \n",
    "                # Neuron h2\n",
    "                self.w[2] -= learn_rate * d_L_d_ypred * d_ypred[3] * d_h1[0]\n",
    "                self.w[3] -= learn_rate * d_L_d_ypred * d_ypred[3] * d_h1[1]\n",
    "                self.b[1] -= learn_rate * d_L_d_ypred * d_ypred[3] * d_h1[2]\n",
    "\n",
    "                # Neuron o1\n",
    "                self.w[4] -= learn_rate * d_L_d_ypred * d_ypred[0]\n",
    "                self.w[5] -= learn_rate * d_L_d_ypred * d_ypred[1]\n",
    "                self.b[2] -= learn_rate * d_L_d_ypred * d_ypred[4]\n",
    "\n",
    "            if epoch % 10 == 0:\n",
    "                y_preds = np.apply_along_axis(self.feedforward, 1, data)\n",
    "                loss = mse_loss(all_y_trues, y_preds)\n",
    "                res += [epoch, loss]\n",
    "        return res\n",
    "\n",
    "\n",
    "# Define dataset\n",
    "data = np.array([\n",
    "    [-2, -1],  # Alice\n",
    "    [25, 6],  # Bob\n",
    "    [17, 4],  # Charlie\n",
    "    [-15, -6],  # Diana\n",
    "])\n",
    "all_y_trues = np.array([\n",
    "    1,  # Alice\n",
    "    0,  # Bob\n",
    "    0,  # Charlie\n",
    "    1,  # Diana\n",
    "])\n",
    "\n",
    "# Train our neural network!\n",
    "network = OurNeuralNetwork()\n",
    "res = network.train(data, all_y_trues)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-14T14:38:51.084983Z",
     "start_time": "2019-12-14T14:38:50.688851Z"
    }
   },
   "outputs": [],
   "source": [
    "_ = plt.plot(np.array(res[0::2]), np.array(res[1::2]))\n",
    "_ = plt.xlabel(\"eporch\")\n",
    "_ = plt.ylabel(\"loss\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-14T12:15:20.732657Z",
     "start_time": "2019-12-14T12:15:20.695993Z"
    }
   },
   "source": [
    "## new version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-19T08:50:06.820630Z",
     "start_time": "2019-12-19T08:50:06.806113Z"
    }
   },
   "outputs": [],
   "source": [
    "dot = lambda w, x, bias: w[0] * x[0] + w[1] * x[1] + bias\n",
    "sigmoid = lambda x: 1 / (1 + np.exp(-x))\n",
    "dsig = lambda x: sigmoid(x) * (1- sigmoid(x))\n",
    "mse_loss = lambda y_true, y_pred: ((y_true - y_pred)**2).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-19T08:50:08.812803Z",
     "start_time": "2019-12-19T08:50:08.387675Z"
    }
   },
   "outputs": [],
   "source": [
    "class OurNeuralNetwork:\n",
    "    def __init__(self):\n",
    "        number_of_weights = 6\n",
    "        number_of_bias = 3\n",
    "\n",
    "        self.w = [0] * number_of_weights\n",
    "        for i in range(1, number_of_weights):\n",
    "            self.w[i] = np.random.normal()\n",
    "\n",
    "        self.b = [0] * number_of_bias\n",
    "        for i in range(0, number_of_bias):\n",
    "            self.b[i] = np.random.normal()\n",
    "\n",
    "    def feedforward(self, x):\n",
    "        h = [0] * 2\n",
    "        h[0] = sigmoid(dot(self.w[0:2], x, self.b[0]))\n",
    "        h[1] = sigmoid(dot(self.w[2:4], x, self.b[1]))\n",
    "        o1 = sigmoid(dot(self.w[4:6], h, self.b[2]))\n",
    "        return o1\n",
    "\n",
    "    def train(self, data, all_y_trues):\n",
    "        learn_rate = 0.1\n",
    "        epochs = 1000\n",
    "        res = []\n",
    "        for epoch in range(epochs):\n",
    "            for x, y_true in zip(data, all_y_trues):\n",
    "\n",
    "                s = [0] * 3\n",
    "                h = [0] * 2\n",
    "                for i in range(2):\n",
    "                    s[i] = dot(self.w[2*i : 2*(i + 1)], x, self.b[i])\n",
    "                    h[i] = sigmoid(s[i])\n",
    "\n",
    "                s[2] = dot(self.w[4:6], h, self.b[2])\n",
    "                y_pred = sigmoid(s[2])\n",
    "\n",
    "                d_L_d_ypred = -2 * (y_true - y_pred)\n",
    "                \n",
    "                \"\"\"\n",
    "                ww = sympy.Array([abc.a, abc.b])\n",
    "                xx = sympy.Array([abc.m, abc.n])\n",
    "                bias = abc.kappa\n",
    "                \n",
    "                table = sympy.derive_by_array(dot(ww, xx, bias), [ww[0], ww[1], xx[0], xx[1], bias])\n",
    "               \n",
    "                table2 = table.subs({\n",
    "                        ww[0]: self.w[4],\n",
    "                        ww[1]: self.w[5],\n",
    "                        xx[0]: h[0],\n",
    "                        xx[1]: h[1],\n",
    "                        bias: self.b[2]\n",
    "                    })\n",
    "                \"\"\"\n",
    "                table2 = [h[0], h[1], self.w[4], self.w[5], 1]\n",
    "                \n",
    "                \n",
    "                d_ypred = [0] * 5\n",
    "                for i in range(5):\n",
    "                    d_ypred[i] = table2[i] * dsig(s[2])\n",
    "                \"\"\"\n",
    "                table = sympy.derive_by_array(dot(ww, xx, bias), [ww[0], ww[1], bias])\n",
    "                table1 = table.subs({xx[0]: x[0], xx[1]: x[1], bias: self.b[1]}) \n",
    "                table0 = table.subs({xx[0]: x[0], xx[1]: x[1], bias: self.b[0]}) \n",
    "                \"\"\"\n",
    "                table1 = [self.w[2], self.w[3], 1]\n",
    "                table0 = [self.w[0], self.w[1], 1]\n",
    "                d_h0 = [0] * 3\n",
    "                d_h1 = [0] * 3\n",
    "                for i in range(3):\n",
    "                    d_h0[i] = table0[i] * dsig(s[0])\n",
    "                    d_h1[i] = table1[i] * dsig(s[1])\n",
    "                \n",
    "                # Neuron h1\n",
    "                self.w[0] -= learn_rate * d_L_d_ypred * d_ypred[2] * d_h0[0]\n",
    "                self.w[1] -= learn_rate * d_L_d_ypred * d_ypred[2] * d_h0[1]\n",
    "                self.b[0] -= learn_rate * d_L_d_ypred * d_ypred[2] * d_h0[2]\n",
    "                \n",
    "                # Neuron h2\n",
    "                self.w[2] -= learn_rate * d_L_d_ypred * d_ypred[3] * d_h1[0]\n",
    "                self.w[3] -= learn_rate * d_L_d_ypred * d_ypred[3] * d_h1[1]\n",
    "                self.b[1] -= learn_rate * d_L_d_ypred * d_ypred[3] * d_h1[2]\n",
    "\n",
    "                # Neuron o1\n",
    "                self.w[4] -= learn_rate * d_L_d_ypred * d_ypred[0]\n",
    "                self.w[5] -= learn_rate * d_L_d_ypred * d_ypred[1]\n",
    "                self.b[2] -= learn_rate * d_L_d_ypred * d_ypred[4]\n",
    "\n",
    "            if epoch % 10 == 0:\n",
    "                y_preds = np.apply_along_axis(self.feedforward, 1, data)\n",
    "                loss = mse_loss(all_y_trues, y_preds)\n",
    "                res += [epoch, loss]\n",
    "        return res\n",
    "\n",
    "\n",
    "# Define dataset\n",
    "data = np.array([\n",
    "    [-2, -1],  # Alice\n",
    "    [25, 6],  # Bob\n",
    "    [17, 4],  # Charlie\n",
    "    [-15, -6],  # Diana\n",
    "])\n",
    "all_y_trues = np.array([\n",
    "    1,  # Alice\n",
    "    0,  # Bob\n",
    "    0,  # Charlie\n",
    "    1,  # Diana\n",
    "])\n",
    "\n",
    "# Train our neural network!\n",
    "network = OurNeuralNetwork()\n",
    "res = network.train(data, all_y_trues)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-19T08:53:37.856433Z",
     "start_time": "2019-12-19T08:53:37.640311Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "_ = plt.plot(np.array(res[0::2]), np.array(res[1::2]))\n",
    "_ = plt.xlabel(\"eporch\")\n",
    "_ = plt.ylabel(\"loss\")"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
