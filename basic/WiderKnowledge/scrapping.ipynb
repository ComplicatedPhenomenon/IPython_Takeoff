{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import sys "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "import bs4\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scrapping "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Looking for tags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://stackoverflow.com/questions/57717303/unable-to-find-the-class-for-price-web-scraping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-family:New York Times; font-size:1em; color:red;\">\n",
    "You don't need BeautifulSoup to the data returned by the API is partly in JSON format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "link = 'https://api.microsofttranslator.com/v2/ajax.svc/TranslateArray?appId=%22Tsisu8GHc7uGpQ7QYGl_0cR7OvQduAHZHxuqcIC-6dA8*%22&texts=[%22zaman%C4%B1n%20k%C4%B1sa%20tarihi%20adl%C4%B1%20kitap%20hakk%C4%B1nda%20ne%20d%C3%BC%C5%9F%C3%BCn%C3%BCyorsun%22]&from=%22tr%22&to=%22en%22&oncomplete=_mstc12&onerror=_mste12&loc=en&ctr=&ref=WidgetV2&rgp=20860894'\n",
    "bowll = requests.get(link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = json.loads(bowll.text.split('(')[1].split(')')[0])\n",
    "print(data[0]['TranslatedText'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://stackoverflow.com/questions/57767188/python-beautifulsoup-replace-links-with-url-in-string#57767188"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "html=\"\"\"\n",
    "<html><head></head>\n",
    "<body>\n",
    "<a href=\"www.google.com\">foo</a> some text \n",
    "<a href=\"www.bing.com\">bar</a> some <br> text\n",
    "</body></html>\"\"\"\n",
    "\n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "for a_tag in soup.find_all('a'):\n",
    "    a_tag.string = a_tag.get('href')\n",
    "print(soup)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## http://www.aaronsw.com\n",
    "http://www.aaronsw.com/weblog/fullarchive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bowl = requests.get('http://www.aaronsw.com/weblog/fullarchive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "bowl.text vs bowl.content\n",
    "\n",
    "`bowl.text` is the content of the response in Unicode, and `bowl.content` is the content of the response in bytes. useful when the address refers to such as image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bowl.status_code # bowl.status_code\n",
    "bowl.headers\n",
    "bowl.encoding # 'utf8'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(bowl.text, 'html.parser')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-family:New York Times; font-size:1em; color:green;\">\n",
    "Extract all the url of posts from soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.html.body.p "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.body.find_all('a', href = True, limit=10)[1]['href']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PostLink = soup.body.find_all('a', href = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PostLink = [i['href'] for i in PostLink][2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseurl = 'http://www.aaronsw.com/weblog/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = requests.get(baseurl + PostLink[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "startt = time.time()\n",
    "baseurl = 'http://www.aaronsw.com/weblog/'\n",
    "bowls = [ requests.get(baseurl + i) for i in PostLink]\n",
    "\n",
    "time.time()-startt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(soups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(451):\n",
    "    with open('AaronSwartz/{}.html'.format(i), 'wb') as f:\n",
    "        f.write(BeautifulSoup(bowls[i].text, 'html.parser').encode('utf-8'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-family:New York Times; font-size:1em; color:green;\">\n",
    "It's a good scenario to apply multithreadings\n",
    "    \n",
    "Q: Is the task CPU intensive or I/O intensive? If the answer is I/O intensive, then you can go with threads.\n",
    "\n",
    "https://stackoverflow.com/questions/40894487/python-threading-or-multiprocessing-for-web-crawler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "startt = time.time()\n",
    "pool = ThreadPoolExecutor(6)\n",
    "res = []\n",
    "future = pool.submit(requests.get, [baseurl+i for i in PostLink])\n",
    "time.time()-startt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing import Pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "startt = time.time()\n",
    "with Pool(6) as p:\n",
    "    bowls = p.map(requests.get, [baseurl+i for i in PostLink])\n",
    "time.time()-startt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  http://arxiv.org"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Paper():\n",
    "    \"\"\" A class that holds the information for an Arxiv paper. \"\"\"\n",
    "\n",
    "    def __init__(self, number=None, title=None, auths=None,abstract=None,fromfile=None):\n",
    "        \"\"\" Initialize a paper with the arxiv number, title, authors, and abstract. \"\"\"\n",
    "\n",
    "        if fromfile is not None:\n",
    "            self.load(fromfile)\n",
    "\n",
    "        else:\n",
    "            self.number = number\n",
    "            self.title = title\n",
    "            if auths is not None:\n",
    "                self.authors = list(auths.values())\n",
    "                self.author_ids = list(auths.keys())\n",
    "                self.author_dict = auths.copy()\n",
    "            else:\n",
    "                self.authors = None\n",
    "                self.author_ids = None\n",
    "                self.author_dict = None\n",
    "\n",
    "            self.abstract = abstract\n",
    "            self.link = u'http://arxiv.org/abs/' + number\n",
    "\n",
    "    def format_line(self,strval, maxlength,pad_left,pad_right):\n",
    "        \"\"\" Function to format a line of a given length.\n",
    "        Used by the __str__ routine.\"\"\"\n",
    "        temp = re.sub(\"(.{\" + \"{:d}\".format(maxlength) + \"})\", u\"\\\\1-\\n\", strval.replace('\\n',''), 0, re.DOTALL).strip()\n",
    "\n",
    "        temp = temp.split('\\n')\n",
    "\n",
    "        temp[-1] = temp[-1] +''.join([u'\\u0020']*(maxlength-len(temp[-1])))\n",
    "        if len(temp) > 1:\n",
    "            temp[0] = temp[0][:-1]+temp[0][-1]\n",
    "\n",
    "        return pad_left + (pad_right + '\\n' + pad_left).join(temp) + pad_right\n",
    "\n",
    "    def get_search_string(self):\n",
    "\n",
    "        return '  '.join([self.abstract.lower(),self.title.lower(), self.number] + [a.lower() for a in self.author_ids] +  [a.lower() for a in self.authors])\n",
    "\n",
    "    def save(self,filename):\n",
    "        with open(filename,\"a\") as f:\n",
    "            json.dump(vars(self),f)\n",
    "    def load(self,filename):\n",
    "        try:\n",
    "            if os.path.exists(filename):\n",
    "                with open(filename, 'r') as f:\n",
    "                    dat = json.load(f)\n",
    "            else:\n",
    "                dat = filename\n",
    "        except TypeError:\n",
    "            dat = filename\n",
    "        for key,val in dat.items():\n",
    "            setattr(self,key,val)\n",
    "\n",
    "\n",
    "    def __eq__(self,paper):\n",
    "        return (self.number == paper.number)\n",
    "\n",
    "    def __ne__(self,paper):\n",
    "        return not self.__eq__(paper)\n",
    "\n",
    "    def __le__(self,paper):\n",
    "        return float(self.number) <= float(paper.number)\n",
    "    def __ge__(self,paper):\n",
    "        return float(self.number) >= float(paper.number)\n",
    "    def __lt__(self,paper):\n",
    "        return float(self.number) <  float(paper.number)\n",
    "    def __gt__(self,paper):\n",
    "        return float(self.number) >  float(paper.number)\n",
    "\n",
    "    def __str__(self):\n",
    "        \"\"\" Display the paper in a somewhat nice looking way. \"\"\"\n",
    "\n",
    "        maxlen = 80\n",
    "        pad_char = u\"\\u0025\"\n",
    "        newline_char = u\"\\u000A\"\n",
    "        space_char = u\"\\u0020\"\n",
    "        tab_char = space_char + space_char + space_char + space_char\n",
    "        comma_char = u\"\\u002C\"\n",
    "        and_char = u\"\\u0026\"\n",
    "\n",
    "\n",
    "        pad_left = pad_char + pad_char + pad_char + tab_char\n",
    "        pad_right = tab_char + pad_char + pad_char + pad_char\n",
    "\n",
    "        if len(self.authors) == 1:\n",
    "            authstr = self.authors[0]\n",
    "        else:\n",
    "            authstr = (comma_char + space_char).join(self.authors[:-1])\n",
    "            authstr += comma_char + space_char + and_char + space_char + self.authors[-1]\n",
    "\n",
    "        authstr  = self.format_line(authstr,  maxlen, pad_left, pad_right)\n",
    "        titlestr = self.format_line(self.title, maxlen, pad_left, pad_right)\n",
    "        linkstr  = self.format_line(self.link, maxlen, pad_left, pad_right)\n",
    "        border = ''.join([pad_char]*(maxlen + len(pad_left) + len(pad_right)))\n",
    "        blank_line = pad_left + ''.join([space_char] * maxlen) + pad_right\n",
    "\n",
    "\n",
    "        strbody = newline_char + \\\n",
    "                border + newline_char + \\\n",
    "                blank_line  + newline_char + \\\n",
    "                titlestr + newline_char + \\\n",
    "                blank_line  + newline_char + \\\n",
    "                linkstr + newline_char + \\\n",
    "                blank_line  + newline_char + \\\n",
    "                authstr + newline_char + \\\n",
    "                blank_line  + newline_char + \\\n",
    "                border + newline_char + \\\n",
    "                newline_char\n",
    "\n",
    "        # Check for python 2 to convert from unicode\n",
    "        if sys.version_info < (3,):\n",
    "            strbody = strbody.encode(\"utf8\",\"ignore\")\n",
    "        return strbody"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def authors_list_to_dict(author_list):\n",
    "\n",
    "    authors_dict = {}\n",
    "    for a in author_list:\n",
    "\n",
    "        if '(' in a:\n",
    "            # We have an affiliation\n",
    "            a = a.split('(')[0]\n",
    "            #a = ' ' .join(a.split('(')[0])\n",
    "        temp = a.split()\n",
    "\n",
    "        if len(temp) > 2:\n",
    "            # More than two names, take first and last\n",
    "            name = (temp[0],temp[-1])\n",
    "        elif len(temp) == 1:\n",
    "            # Just one name, probably a spacing error\n",
    "            temp = temp[0].split('.')\n",
    "            name = (temp[0],temp[-1])\n",
    "        else:\n",
    "            # Two names\n",
    "            name = (temp[0],temp[1])\n",
    "\n",
    "        authors_dict[name[1]+'_'+name[0][0].upper()] = ' '.join(temp)\n",
    "    return authors_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_paper_from_url(number):\n",
    "\n",
    "    bowl = requests.get('http://arxiv.org/abs/' + str(number))\n",
    "    soup = bs4.BeautifulSoup(bowl.text, 'html.parser')\n",
    "    title = soup.find_all(\n",
    "        'h1', attrs={'class':\n",
    "                     'title mathjax'})[0].text.split('Title:')[-1].strip()\n",
    "\n",
    "    authors = [\n",
    "        x.strip() for x in soup.find_all('div', attrs={'class': 'authors'})[0].\n",
    "        text.split('Authors:')[-1].split(',')\n",
    "    ]\n",
    "\n",
    "    abstract = soup.find_all(\n",
    "        'blockquote',\n",
    "        attrs={'class':\n",
    "               'abstract mathjax'})[0].text.split('Abstract:')[-1].strip()\n",
    "\n",
    "    return Paper(number, title, authors_list_to_dict(authors), abstract)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "read_paper_from_url('1908.04905')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you’ve captured the return value of get() by bowl\n",
    "bowl = requests.get('http://arxiv.org/abs/'+ str(1908.04905)) \n",
    "soup = bs4.BeautifulSoup(bowl.text, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('arxiv.html', 'wb') as f:\n",
    "    f.write(soup.encode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%html \n",
    "<h1 class=\"title mathjax\"><span class=\"descriptor\">Title:</span>Random walk on a lattice in the presence of obstacles: The short-time transient regime, anomalous diffusion and crowding</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = bs4.BeautifulSoup(bowl.text, 'html.parser')\n",
    "#soup\n",
    "title = soup.find_all(\n",
    "        'h1', attrs={'class':\n",
    "                     'title mathjax'})[0].text.split('Title:')[-1].strip()\n",
    "title\n",
    "authors = [\n",
    "        x.strip() for x in soup.find_all('div', attrs={'class': 'authors'})[0].\n",
    "        text.split('Authors:')[-1].split(',')\n",
    "    ]\n",
    "authors\n",
    "abstract = soup.find_all('blockquote',attrs={'class':\n",
    "               'abstract mathjax'})[0].text.split('Abstract:')[-1].strip()\n",
    "abstract"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## https://www.gotouniversity.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  `bs4`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bowl = requests.get('https://www.gotouniversity.com/course/index') \n",
    "soup = bs4.BeautifulSoup(bowl.text, 'html.parser')\n",
    "UniversityName = [i.text for i in soup.find_all('p', attrs={'class': 'university-name'})]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When writing a scraper, it's a good idea to look at the source of the HTML file and familiarize yourself with the structure. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('gotouniversity.html', 'wb') as f:\n",
    "    f.write(soup.encode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%html \n",
    "<a href=\"https://www.gotouniversity.com/programs/accelerated-bachelors/united-states-of-america/sciences/biochemistry-and-biophysics/loyola-university-chicago/bsms-in-biochemistry\" target=\"_blank\">\n",
    "<span class=\"large-text program-name\" title=\"BSMS in Biochemistry\">BSMS in Biochemistry</span>\n",
    "</a>\n",
    "<script type=\"application/ld+json\">\n",
    "{\n",
    "  \"@context\": \"http://schema.org\",\n",
    "  \"@type\": \"Course\",\n",
    "  \"name\": \"BSMS in Biochemistry\",\n",
    "  \"description\": \"\",\n",
    "  \"provider\": {\n",
    "    \"@type\": \"Organization\",\n",
    "    \"name\": \"Loyola University Chicago\",\n",
    "    \"sameAs\": \"https://www.gotouniversity.com/university/loyola-university-chicago\"\n",
    "  }\n",
    "}\n",
    "</script>\n",
    "<a href=\"/university/loyola-university-chicago\" target=\"_blank\" title=\"University\">\n",
    "<p class=\"university-name\" title=\"Loyola University Chicago\">Loyola University Chicago</p>\n",
    "</a>\n",
    "<p class=\"location-name\" title=\"Chicago Illinois\"> Chicago, Illinois</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://www.gotouniversity.com/course/index'\n",
    "\n",
    "params = {'page': 80}\n",
    "UniversityName = []\n",
    "ProjectName = []\n",
    "for page in range(1, 11):\n",
    "    \n",
    "    params['page'] = page\n",
    "    soup = BeautifulSoup( requests.post(url, data=params).text, 'html.parser' )\n",
    "\n",
    "    UniversityName.append([a.get_text(strip=True) for a in soup.select('a[title=\"University\"]') ])\n",
    "    ProjectName.append([a.get_text(strip=True) for a in soup.select('span[class=\"large-text program-name\"]')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "UniversityName = sum(UniversityName, [])\n",
    "ProjectName = sum(ProjectName, [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://www.gotouniversity.com/course/index'\n",
    "params = {'page': 80}\n",
    "\n",
    "for page in range(1, 2):\n",
    "    \n",
    "    params['page'] = page\n",
    "    soup = BeautifulSoup( requests.post(url, data=params).text, 'html.parser' )\n",
    "    print([a.get_text(strip=True) for a in soup.select('a[title=\"University\"]')] )\n",
    "    print([a.get_text(strip=True) for a in soup.select('span[class=\"large-text program-name\"]')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = UniversityName[1]\n",
    "y = ProjectName[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\"UniversityName\": x, \"ProjectName\": y}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data)\n",
    "df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "UniversityName = sum(UniversityName, [])\n",
    "ProjectName = sum(ProjectName, [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\"ProjectName\": ProjectName, \"UniversityName\": UniversityName}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data)\n",
    "df.to_json('gotoun.json') \n",
    "df.to_excel('gotoun.xlsx', engine='xlsxwriter')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### selenium"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://stackoverflow.com/questions/51591849/scraping-javascriptvoid0-content-using-python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-family:New York Times; font-size:1em; color:green;\">\n",
    "The only way to do this is to execute the Javascript that handles the click event - you won't do it with a regular GET request."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://stackoverflow.com/questions/31442119/mechanize-and-python-clicking-href-javascriptvoid0-links-and-getting-the"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "driver = webdriver.Chrome(executable_path='/Users/wangmiao/Desktop/chromedriver')\n",
    "# find the element that's name attribute is q (the google search box)\n",
    "driver.get('https://www.gotouniversity.com/course/index')\n",
    "university_name = driver.find_elements_by_class_name(\"university-name\")\n",
    "university_name = [link.text for link in university_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "WebDriverException: Message: 'chromedriver' executable needs to be in PATH. Please see https://sites.google.com/a/chromium.org/chromedriver/home\n",
    "\n",
    "https://stackoverflow.com/questions/40555930/selenium-chromedriver-executable-needs-to-be-in-path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! /Applications/Google\\ Chrome.app/Contents/MacOS/Google\\ Chrome --version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-family:New York Times; font-size:1em; color:green;\">\n",
    "    \n",
    "如果href链接`(<a>)`值是javascript:void(0) 而不是网址，暗示链接不用于href响应页面导航，但使用绑定到链接点击事件的javascript函数来响应用户点击。因此无法driver.get(url)直接打开目标页面，必须单击链接以触发单击事件，该事件将调用javascript函数以导航到目标页面。\n",
    "\n",
    "提示需要等待一段时间才能完成浏览器加载javascript并注册javascript函数以链接点击事件。否则点击链接后没有任何反应。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%html\n",
    "<div class=\"pagination\"><div aria-live=\"polite\" role=\"status\" style=\"float:left; height:14px; padding:8px\">Showing 1 to 20 of 143981 entries</div><div style=\"float:right;\"><ul class=\"pagination\" id=\"pagin_count\"><li class=\"active\" p=\"1\"><a>1</a></li><li p=\"2\"><a href=\"javascript:void()\" onclick=\"pagingcustom(2);\">2</a></li><li p=\"3\"><a href=\"javascript:void()\" onclick=\"pagingcustom(3);\">3</a></li><li p=\"4\"><a href=\"javascript:void()\" onclick=\"pagingcustom(4);\">4</a></li><li p=\"5\"><a href=\"javascript:void()\" onclick=\"pagingcustom(5);\">5</a></li><li p=\"6\"><a href=\"javascript:void()\" onclick=\"pagingcustom(6);\">6</a></li><li p=\"7\"><a href=\"javascript:void()\" onclick=\"pagingcustom(7);\">7</a></li><li p=\"8\"><a href=\"javascript:void()\" onclick=\"pagingcustom(8);\">8</a></li><li p=\"9\"><a href=\"javascript:void()\" onclick=\"pagingcustom(9);\">9</a></li><li p=\"10\"><a href=\"javascript:void()\" onclick=\"pagingcustom(10);\">10</a></li><li p=\"1\"><a href=\"javascript:void()\" onclick=\"pagingcustom(1);\">Next</a></li></ul></div></div>\n",
    "</div>\n",
    "<script>\n",
    "function fn_advcount(id){\n",
    "    $.ajax({\n",
    "            url: 'https://www.gotouniversity.com/site/advertisement-count',\n",
    "            data: { id : id },\n",
    "            success: function(result){\n",
    "    }});\n",
    "  }\n",
    "</script>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-family:New York Times; font-size:1em; color:green;\">\n",
    "\"javascript:void(0)\" means that the link wouldn't work. It'll do nothing. That is why no action is taking place when you click on it.\n",
    "    \n",
    "https://stackoverflow.com/a/1291950/7583919"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/a/35786344/7583919\n",
    "aElements = driver.find_elements_by_tag_name(\"a\")\n",
    "result = []\n",
    "for name in aElements:\n",
    "    if(name.get_attribute(\"href\") is not None and \"javascript:void()\" in name.get_attribute(\"href\")):\n",
    "        print(\"IM IN HUR\")\n",
    "        \"\"\"\n",
    "        elements = driver.find_elements_by_class_name(\"university-name\")\n",
    "        result.append([link.text for link in elements])\n",
    "        print(result)\n",
    "        \"\"\"\n",
    "        name.click()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://codeday.me/bug/20190123/563610.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://stackoverflow.com/questions/52876136/google-search-next-pages-using-selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "driver = webdriver.Chrome(executable_path='/Users/wangmiao/Desktop/chromedriver')\n",
    "driver.get('https://www.gotouniversity.com/course/index')\n",
    "Page_number = 1\n",
    "Max_page = 10\n",
    "\n",
    "while Page_number <= Max_page:\n",
    "\n",
    "    university_name = WebDriverWait(driver, 20).until(\n",
    "        EC.presence_of_all_elements_located((By.CSS_SELECTOR,\n",
    "                                             '.university-name')))\n",
    "    university_name = [link.text for link in university_name]\n",
    "    print(university_name)\n",
    "    Page_number = Page_number + 1\n",
    "    element = WebDriverWait(driver, 20).until(\n",
    "        EC.element_to_be_clickable((By.XPATH,\n",
    "                                    '//a[text()=\"' + str(Page_number) + '\"]')))\n",
    "    driver.execute_script(\"arguments[0].click();\", element)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## https://realpython.com\n",
    "https://realpython.com/modern-web-automation-with-python-and-selenium/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "browser = webdriver.Chrome(executable_path='/Users/wangmiao/Desktop/chromedriver')\n",
    "browser.get('https://duckduckgo.com')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_form = browser.find_element_by_id('search_form_input_homepage')\n",
    "search_form.send_keys('real python')\n",
    "search_form.submit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = browser.find_elements_by_class_name('result')\n",
    "print(results[10].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "browser.close()\n",
    "quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## https://www.premierleague.com/players\n",
    "\n",
    "Besides getting the content parsed in html, there is other format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup as soup\n",
    "from pprint import pprint\n",
    "\n",
    "player_name = [\n",
    "    'Bernd Leno', 'Emiliano Martínez', 'Matt Macey', 'Héctor Bellerín'\n",
    "]\n",
    "player = {}\n",
    "for i in player_name:\n",
    "    player_page = requests.get(\n",
    "        'https://www.premierleague.com/players/10483/{}/stats'.format(i))\n",
    "    #Set lxml as BeautifulSoup parser\n",
    "    cont = soup(player_page.content, 'lxml')  \n",
    "\n",
    "    data = dict(\n",
    "        (k.contents[0].strip(), v.get_text(strip=True)) for k, v in zip(\n",
    "            cont.select('.topStat span.stat, .normalStat span.stat'),\n",
    "            cont.select(\n",
    "                '.topStat span.stat > span, .normalStat span.stat > span')))\n",
    "    player[i] = data\n",
    "\n",
    "pprint(player)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls &&pwd \n",
    "os.path.isfile(\"/Applications/Users/wangmiao/Playground/GH/IPython_training/basic/WiderKnowledge/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern=re.compile(r\"\\.\\w{5}\")\n",
    "for root, dirs, files in os.walk(\"/Users/wangmiao/Playground/GH/IPython_training/basic/WiderKnowledge/\"):\n",
    "    for file in files:\n",
    "        #if pattern.findall(file)[0] == '.ipynb':\n",
    "        print( file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "mypath = \"/Users/wangmiao/Playground/GH/IPython_training/basic/WiderKnowledge/\"\n",
    "[f for f in listdir(mypath) if isfile(join(mypath, f))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern.findall(\"Decorators.ipynb\")[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `bs4`\n",
    "\n",
    "[A Simple Cheat Sheet for Web Scraping with Python](https://blog.hartleybrody.com/web-scraping-cheat-sheet/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "requests.get(url, params=None, **kwargs)\n",
    "\n",
    "usage example\n",
    "\n",
    "html = requests.get(url, cookies=self.cookie).content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests, bs4\n",
    "\n",
    "url = 'https://www.basketball-reference.com/players/a/abrinal01.html'\n",
    "res = requests.get(url)\n",
    "res.raise_for_status()\n",
    "\n",
    "soup = bs4.BeautifulSoup(res.text, 'html.parser')\n",
    "elems = soup.select('#per_game')\n",
    "\n",
    "table = soup.find(\"table\", { \"id\" : \"per_game\" })\n",
    "table_rows = table.find_all('tr')\n",
    "\n",
    "for tr in table_rows:\n",
    "    td = tr.find_all('td') + tr.find_all('th')\n",
    "    row = [i.text for i in td]\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract from  `xml`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 统招本科以上学历，优秀的编码与代码控制能力, 有扎实的数据结构和算法功底；\n",
    "2. 良好的逻辑思维能力，能够从海量数据中发现关键特征；\n",
    "3. 熟悉linux开发环境，scrapy等爬虫原理和相关技术，熟悉C++或Java和python语言；\n",
    "4. 熟悉爬虫原理，熟悉常见的反爬虫技术；\n",
    "5. 掌握http协议，熟悉html、dom、xpath等常见的数据抽取技术；\n",
    "6. 对Hadoop、Spark等大规模数据存储与运算平台有丰富实践经验，熟悉分布式计算；\n",
    "7. 有大规模数据处理、数据挖掘、信息提取等经验者优先。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://docs.python.org/3/library/xml.etree.elementtree.html\n",
    "\n",
    "A good tool with an explicit documentation about its API would get your work done soon. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xml.etree import cElementTree as ET"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We can import this data by reading from a file\n",
    "* Or directly from a string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-family:New York Times; font-size:1.2em; color:red;\">\n",
    "    \n",
    "the `<poll>` element contains a couple of \"attributes\", such as `title` `totalvotes` `name` that give even more information!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'poll'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "{'title': 'User Suggested Number of Players',\n",
       " 'totalvotes': '0',\n",
       " 'name': 'suggested_numplayers'}"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "['title', 'totalvotes', 'name']"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "[<Element 'poll' at 0x10a6a8e58>]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "[<Element 'results' at 0x10a6a89a8>]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xmlstr = \"\"\"<poll title=\"User Suggested Number of Players\" totalvotes=\"0\" name=\"suggested_numplayers\">\n",
    "<results numplayers=\"3+\"> \n",
    "</results></poll>\n",
    "\"\"\"\n",
    "root = ET.fromstring(xmlstr)\n",
    "root.tag\n",
    "root.attrib\n",
    "root.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root.findall('.')\n",
    "root.findall('.//')\n",
    "root.findall('.//')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source = '''<root>\n",
    "<level>\n",
    "  <name>Matthias</name>\n",
    "  <age>23</age>\n",
    "  <gender>Male</gender>\n",
    "</level>\n",
    "<level>\n",
    "  <name>Foo</name>\n",
    "  <age>24</age>\n",
    "  <gender>Male</gender>\n",
    "</level>\n",
    "<level>\n",
    "  <name>Bar</name>\n",
    "  <age>25</age>\n",
    "  <gender>Male</gender>\n",
    "</level>\n",
    "</root>'''\n",
    "\n",
    "root = ET.fromstring(source)\n",
    "levels = root.findall('level')\n",
    "for level in levels:\n",
    "    name = level.find('name').text\n",
    "    age = level.find('age').text\n",
    "    print(name, age)"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
