{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# requests to arxiv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import sys "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "import bs4\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Paper():\n",
    "    \"\"\" A class that holds the information for an Arxiv paper. \"\"\"\n",
    "\n",
    "    def __init__(self, number=None, title=None, auths=None,abstract=None,fromfile=None):\n",
    "        \"\"\" Initialize a paper with the arxiv number, title, authors, and abstract. \"\"\"\n",
    "\n",
    "        if fromfile is not None:\n",
    "            self.load(fromfile)\n",
    "\n",
    "        else:\n",
    "            self.number = number\n",
    "            self.title = title\n",
    "            if auths is not None:\n",
    "                self.authors = list(auths.values())\n",
    "                self.author_ids = list(auths.keys())\n",
    "                self.author_dict = auths.copy()\n",
    "            else:\n",
    "                self.authors = None\n",
    "                self.author_ids = None\n",
    "                self.author_dict = None\n",
    "\n",
    "            self.abstract = abstract\n",
    "            self.link = u'http://arxiv.org/abs/' + number\n",
    "\n",
    "    def format_line(self,strval, maxlength,pad_left,pad_right):\n",
    "        \"\"\" Function to format a line of a given length.\n",
    "        Used by the __str__ routine.\"\"\"\n",
    "        temp = re.sub(\"(.{\" + \"{:d}\".format(maxlength) + \"})\", u\"\\\\1-\\n\", strval.replace('\\n',''), 0, re.DOTALL).strip()\n",
    "\n",
    "        temp = temp.split('\\n')\n",
    "\n",
    "        temp[-1] = temp[-1] +''.join([u'\\u0020']*(maxlength-len(temp[-1])))\n",
    "        if len(temp) > 1:\n",
    "            temp[0] = temp[0][:-1]+temp[0][-1]\n",
    "\n",
    "        return pad_left + (pad_right + '\\n' + pad_left).join(temp) + pad_right\n",
    "\n",
    "    def get_search_string(self):\n",
    "\n",
    "        return '  '.join([self.abstract.lower(),self.title.lower(), self.number] + [a.lower() for a in self.author_ids] +  [a.lower() for a in self.authors])\n",
    "\n",
    "    def save(self,filename):\n",
    "        with open(filename,\"a\") as f:\n",
    "            json.dump(vars(self),f)\n",
    "    def load(self,filename):\n",
    "        try:\n",
    "            if os.path.exists(filename):\n",
    "                with open(filename, 'r') as f:\n",
    "                    dat = json.load(f)\n",
    "            else:\n",
    "                dat = filename\n",
    "        except TypeError:\n",
    "            dat = filename\n",
    "        for key,val in dat.items():\n",
    "            setattr(self,key,val)\n",
    "\n",
    "\n",
    "    def __eq__(self,paper):\n",
    "        return (self.number == paper.number)\n",
    "\n",
    "    def __ne__(self,paper):\n",
    "        return not self.__eq__(paper)\n",
    "\n",
    "    def __le__(self,paper):\n",
    "        return float(self.number) <= float(paper.number)\n",
    "    def __ge__(self,paper):\n",
    "        return float(self.number) >= float(paper.number)\n",
    "    def __lt__(self,paper):\n",
    "        return float(self.number) <  float(paper.number)\n",
    "    def __gt__(self,paper):\n",
    "        return float(self.number) >  float(paper.number)\n",
    "\n",
    "    def __str__(self):\n",
    "        \"\"\" Display the paper in a somewhat nice looking way. \"\"\"\n",
    "\n",
    "        maxlen = 80\n",
    "        pad_char = u\"\\u0025\"\n",
    "        newline_char = u\"\\u000A\"\n",
    "        space_char = u\"\\u0020\"\n",
    "        tab_char = space_char + space_char + space_char + space_char\n",
    "        comma_char = u\"\\u002C\"\n",
    "        and_char = u\"\\u0026\"\n",
    "\n",
    "\n",
    "        pad_left = pad_char + pad_char + pad_char + tab_char\n",
    "        pad_right = tab_char + pad_char + pad_char + pad_char\n",
    "\n",
    "        if len(self.authors) == 1:\n",
    "            authstr = self.authors[0]\n",
    "        else:\n",
    "            authstr = (comma_char + space_char).join(self.authors[:-1])\n",
    "            authstr += comma_char + space_char + and_char + space_char + self.authors[-1]\n",
    "\n",
    "        authstr  = self.format_line(authstr,  maxlen, pad_left, pad_right)\n",
    "        titlestr = self.format_line(self.title, maxlen, pad_left, pad_right)\n",
    "        linkstr  = self.format_line(self.link, maxlen, pad_left, pad_right)\n",
    "        border = ''.join([pad_char]*(maxlen + len(pad_left) + len(pad_right)))\n",
    "        blank_line = pad_left + ''.join([space_char] * maxlen) + pad_right\n",
    "\n",
    "\n",
    "        strbody = newline_char + \\\n",
    "                border + newline_char + \\\n",
    "                blank_line  + newline_char + \\\n",
    "                titlestr + newline_char + \\\n",
    "                blank_line  + newline_char + \\\n",
    "                linkstr + newline_char + \\\n",
    "                blank_line  + newline_char + \\\n",
    "                authstr + newline_char + \\\n",
    "                blank_line  + newline_char + \\\n",
    "                border + newline_char + \\\n",
    "                newline_char\n",
    "\n",
    "        # Check for python 2 to convert from unicode\n",
    "        if sys.version_info < (3,):\n",
    "            strbody = strbody.encode(\"utf8\",\"ignore\")\n",
    "        return strbody"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def authors_list_to_dict(author_list):\n",
    "\n",
    "    authors_dict = {}\n",
    "    for a in author_list:\n",
    "\n",
    "        if '(' in a:\n",
    "            # We have an affiliation\n",
    "            a = a.split('(')[0]\n",
    "            #a = ' ' .join(a.split('(')[0])\n",
    "        temp = a.split()\n",
    "\n",
    "        if len(temp) > 2:\n",
    "            # More than two names, take first and last\n",
    "            name = (temp[0],temp[-1])\n",
    "        elif len(temp) == 1:\n",
    "            # Just one name, probably a spacing error\n",
    "            temp = temp[0].split('.')\n",
    "            name = (temp[0],temp[-1])\n",
    "        else:\n",
    "            # Two names\n",
    "            name = (temp[0],temp[1])\n",
    "\n",
    "        authors_dict[name[1]+'_'+name[0][0].upper()] = ' '.join(temp)\n",
    "    return authors_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_paper_from_url(number):\n",
    "\n",
    "    bowl = requests.get('http://arxiv.org/abs/' + str(number))\n",
    "    soup = bs4.BeautifulSoup(bowl.text, 'html.parser')\n",
    "    title = soup.find_all(\n",
    "        'h1', attrs={'class':\n",
    "                     'title mathjax'})[0].text.split('Title:')[-1].strip()\n",
    "\n",
    "    authors = [\n",
    "        x.strip() for x in soup.find_all('div', attrs={'class': 'authors'})[0].\n",
    "        text.split('Authors:')[-1].split(',')\n",
    "    ]\n",
    "\n",
    "    abstract = soup.find_all(\n",
    "        'blockquote',\n",
    "        attrs={'class':\n",
    "               'abstract mathjax'})[0].text.split('Abstract:')[-1].strip()\n",
    "\n",
    "    return Paper(number, title, authors_list_to_dict(authors), abstract)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "read_paper_from_url('1908.04905')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you’ve captured the return value of get() by bowl\n",
    "bowl = requests.get('http://arxiv.org/abs/'+ str(1908.04905)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bowl.content;\n",
    "#bowl.text;\n",
    "bowl.headers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = bs4.BeautifulSoup(bowl.text, 'html.parser')\n",
    "#soup\n",
    "title = soup.find_all(\n",
    "        'h1', attrs={'class':\n",
    "                     'title mathjax'})[0].text.split('Title:')[-1].strip()\n",
    "title\n",
    "authors = [\n",
    "        x.strip() for x in soup.find_all('div', attrs={'class': 'authors'})[0].\n",
    "        text.split('Authors:')[-1].split(',')\n",
    "    ]\n",
    "authors\n",
    "abstract = soup.find_all('blockquote',attrs={'class':\n",
    "               'abstract mathjax'})[0].text.split('Abstract:')[-1].strip()\n",
    "abstract"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Request to player"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup as soup\n",
    "from pprint import pprint\n",
    "\n",
    "player_name = [\n",
    "    'Bernd Leno', 'Emiliano Martínez', 'Matt Macey', 'Héctor Bellerín'\n",
    "]\n",
    "player = {}\n",
    "for i in player_name:\n",
    "    player_page = requests.get(\n",
    "        'https://www.premierleague.com/players/10483/{}/stats'.format(i))\n",
    "    cont = soup(player_page.content, 'lxml')\n",
    "\n",
    "    data = dict(\n",
    "        (k.contents[0].strip(), v.get_text(strip=True)) for k, v in zip(\n",
    "            cont.select('.topStat span.stat, .normalStat span.stat'),\n",
    "            cont.select(\n",
    "                '.topStat span.stat > span, .normalStat span.stat > span')))\n",
    "    player[i] = data\n",
    "\n",
    "pprint(player)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls &&pwd \n",
    "os.path.isfile(\"/Applications/Users/wangmiao/Playground/GH/IPython_training/basic/WiderKnowledge/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern=re.compile(r\"\\.\\w{5}\")\n",
    "for root, dirs, files in os.walk(\"/Users/wangmiao/Playground/GH/IPython_training/basic/WiderKnowledge/\"):\n",
    "    for file in files:\n",
    "        #if pattern.findall(file)[0] == '.ipynb':\n",
    "        print( file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "mypath = \"/Users/wangmiao/Playground/GH/IPython_training/basic/WiderKnowledge/\"\n",
    "[f for f in listdir(mypath) if isfile(join(mypath, f))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern.findall(\"Decorators.ipynb\")[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `bs4`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests, bs4\n",
    "\n",
    "url = 'https://www.basketball-reference.com/players/a/abrinal01.html'\n",
    "res = requests.get(url)\n",
    "res.raise_for_status()\n",
    "\n",
    "soup = bs4.BeautifulSoup(res.text, 'html.parser')\n",
    "elems = soup.select('#per_game')\n",
    "\n",
    "table = soup.find(\"table\", { \"id\" : \"per_game\" })\n",
    "table_rows = table.find_all('tr')\n",
    "\n",
    "for tr in table_rows:\n",
    "    td = tr.find_all('td')\n",
    "    row = [i.text for i in td]\n",
    "    \n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests, bs4\n",
    "\n",
    "url = 'https://www.basketball-reference.com/players/a/abrinal01.html'\n",
    "res = requests.get(url)\n",
    "res.raise_for_status()\n",
    "\n",
    "soup = bs4.BeautifulSoup(res.text, 'html.parser')\n",
    "elems = soup.select('#per_game')\n",
    "\n",
    "table = soup.find(\"table\", { \"id\" : \"per_game\" })\n",
    "table_rows = table.find_all('tr')\n",
    "\n",
    "for tr in table_rows:\n",
    "    td = tr.find_all('td') + tr.find_all('th')\n",
    "    row = [i.text for i in td]\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check a line in a file\n",
    "https://stackoverflow.com/questions/57078822/how-to-check-to-see-if-a-certain-line-is-found-before-a-certain-point-in-a-txt-f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "find = 'Is this found'   \n",
    "\n",
    "with open('xx.txt') as old_file:\n",
    "  \n",
    "    lines = old_file.readlines()\n",
    "    print(lines[2])\n",
    "    print(lines)\n",
    "    for line in old_file:\n",
    "        print(line)\n",
    "        if line.startswith(\"This\"):\n",
    "            print(\"line\")\n",
    "    \"\"\"\n",
    "    with open(endfile1, \"w\") as new_file:\n",
    "        for num, line in enumerate(lines,1):\n",
    "            #if line \"This is the\" in line:\n",
    "                line_base = num\n",
    "        for line in lines:\n",
    "            if not find in line.range(1:num):\n",
    "                if line.startswith(\"This is the\"):\n",
    "                    line = newbasecase + line \n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('test.txt') as f:\n",
    "    lines = f.read().split('\\n')\n",
    "left = [line.split('=')[0].strip() for line in lines]\n",
    "right = [line.split('=')[1].strip() for line in lines]\n",
    "print(left)\n",
    "print(right)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract from  `xml`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xml.etree import cElementTree as ET\n",
    "xmlstr = \"\"\"<poll title=\"User Suggested Number of Players\" totalvotes=\"0\" name=\"suggested_numplayers\">\n",
    "<results numplayers=\"3+\"> \n",
    "</results></poll>\n",
    "\"\"\"\n",
    "root = ET.fromstring(xmlstr)\n",
    "levels= root.findall('poll')\n",
    "for level in levels:\n",
    "    totalvotes = level.find('totalvotes').text\n",
    "    print('totalvotes', totalvotes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xmlstr\n",
    "a, b , c= xmlstr.partition(\"totalvotes=\")\n",
    "c.split()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "soup = BeautifulSoup(xmlstr) \n",
    "soup.find('poll').get('totalvotes')\n",
    "for poll in soup.find_all('poll'):\n",
    "    print (poll.get('totalvotes'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source = '''<root>\n",
    "<level>\n",
    "  <name>Matthias</name>\n",
    "  <age>23</age>\n",
    "  <gender>Male</gender>\n",
    "</level>\n",
    "<level>\n",
    "  <name>Foo</name>\n",
    "  <age>24</age>\n",
    "  <gender>Male</gender>\n",
    "</level>\n",
    "<level>\n",
    "  <name>Bar</name>\n",
    "  <age>25</age>\n",
    "  <gender>Male</gender>\n",
    "</level>\n",
    "</root>'''\n",
    "\n",
    "root = ET.fromstring(source)\n",
    "levels = root.findall('.//level')\n",
    "for level in levels:\n",
    "    name = level.find('name').text\n",
    "    age = level.find('age').text\n",
    "    print(name, age)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "largerList = [10,10,10,10,10,0,10,10,10,10,15,15,15,15,15,10,10,0,10,10,12,12,12,0]\n",
    "\n",
    "sublist= [largerList[0]]\n",
    "previous = largerList[0]\n",
    "for item in largerList:\n",
    "    #item \n",
    "    if item != previous:\n",
    "        sublist.append(item)\n",
    "        #sublist\n",
    "        previous = item\n",
    "sublist==[10,0,10,15,10,0,10,12,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import groupby\n",
    "LargerList = [10,10,10,10,10,0,10,10,10,10,15,15,15,15,15,10,10,0,10,10,12,12,12,0]\n",
    "sublists = [k for k, _ in groupby(LargerList)]\n",
    "sublists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "page = requests.get('https://finviz.com/forex_performance.ashx')\n",
    "soup = BeautifulSoup(page.content, 'html.parser')\n",
    "forex = soup.find_all(\"div\", {\"class\": \"content \"})\n",
    "print(forex)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# List file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "from os.path import isdir\n",
    "#files = [file for file in listdir('../storage') if isdir(file)]\n",
    "\n",
    "for file in listdir('/Users/wangmiao/Desktop/'):\n",
    "    print(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!echo \"CONFIG_PATH = '/usr/local/emarking/config/config.ini'\" >> test.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat test.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!sed -i \"33iCONFIG_PATH = '/usr/local/emarking/config/config.ini'\" test.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regex_ = r'^((?!~\\$).)+\\.(?:xlsx?|csv|txt)$'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_files = [f_path + filename for f_path, _, filenames in os.walk('./') \\\n",
    "             for filename in filenames if re.search(regex_, filename)]\n"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
