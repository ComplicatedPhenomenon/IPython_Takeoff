{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-16T09:52:03.279961Z",
     "start_time": "2020-01-16T09:52:03.276314Z"
    }
   },
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-16T09:52:03.778451Z",
     "start_time": "2020-01-16T09:52:03.775759Z"
    }
   },
   "outputs": [],
   "source": [
    "import os \n",
    "import sys "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-16T09:52:03.872838Z",
     "start_time": "2020-01-16T09:52:03.863390Z"
    }
   },
   "outputs": [],
   "source": [
    "import time "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-16T09:52:06.554271Z",
     "start_time": "2020-01-16T09:52:04.450868Z"
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "import bs4\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-16T09:52:06.562694Z",
     "start_time": "2020-01-16T09:52:06.557552Z"
    }
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-16T11:52:43.522144Z",
     "start_time": "2020-01-16T11:52:36.479158Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scrapping "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## https://www.springfieldspringfield.co.uk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-08T06:25:53.248286Z",
     "start_time": "2019-12-08T06:25:47.215790Z"
    }
   },
   "outputs": [],
   "source": [
    "bowl = requests.get('https://www.springfieldspringfield.co.uk/view_episode_scripts.php?tv-show=anne-2017&episode=s02e10')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-08T06:25:53.260635Z",
     "start_time": "2019-12-08T06:25:53.256468Z"
    }
   },
   "outputs": [],
   "source": [
    "bowl.status_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-08T07:18:41.617151Z",
     "start_time": "2019-12-08T07:18:41.569140Z"
    }
   },
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(bowl.text, 'html.parser')\n",
    "type(soup)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using a tag name as an attribute will give you only the first tag by that name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-08T06:40:22.884954Z",
     "start_time": "2019-12-08T06:40:22.775982Z"
    }
   },
   "outputs": [],
   "source": [
    "soup.body.div"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-08T06:56:15.815134Z",
     "start_time": "2019-12-08T06:56:15.808741Z"
    }
   },
   "outputs": [],
   "source": [
    "soup.body.find_all('class=\"episode_script\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-08T07:11:35.147311Z",
     "start_time": "2019-12-08T07:11:35.142185Z"
    }
   },
   "source": [
    "get results between `div` and a specific `class`, such as `<div class=\"scrolling-script-container\">`\n",
    "\n",
    "```html\n",
    "<div class=\"episode_script\">\n",
    "<div class=\"scrolling-script-container\">\n",
    "                    \t\t\t1\n",
    " [NO AUDIBLE DIALOGUE.]<br/>\n",
    "  - [MUFFLED.]<br/> Here.<br\n",
    " ...                            \n",
    "</div>  \n",
    "\n",
    "```  \n",
    "https://stackoverflow.com/a/22735249/7583919"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-11T10:33:05.703612Z",
     "start_time": "2019-12-11T10:33:05.571625Z"
    }
   },
   "outputs": [],
   "source": [
    "lyrics = soup.find_all(lambda tag: tag.name == 'div' and tag.get('class') ==\n",
    "                       ['scrolling-script-container'])[0].text\n",
    "\n",
    "subtitle = os.getcwd() + '/subtitle/'\n",
    "if not os.path.exists(subtitle):\n",
    "    os.makedirs(subtitle)\n",
    "\n",
    "with open('subtitle/lyrics.html', 'w') as f:\n",
    "    f.write(lyrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-08T07:19:33.101297Z",
     "start_time": "2019-12-08T07:19:27.605412Z"
    }
   },
   "outputs": [],
   "source": [
    "bowl = requests.get(\n",
    "    'https://www.springfieldspringfield.co.uk/episode_scripts.php?tv-show=anne-2017'\n",
    ")\n",
    "soup = BeautifulSoup(bowl.text, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-08T07:31:22.325617Z",
     "start_time": "2019-12-08T07:31:22.288331Z"
    }
   },
   "outputs": [],
   "source": [
    "soup.body"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-08T07:34:00.425256Z",
     "start_time": "2019-12-08T07:34:00.412288Z"
    }
   },
   "outputs": [],
   "source": [
    "soup.find_all(lambda tag: tag.name == 'div' and tag.get('class') ==\n",
    "              ['main-content-left'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-08T07:34:29.350791Z",
     "start_time": "2019-12-08T07:34:29.340107Z"
    }
   },
   "outputs": [],
   "source": [
    "soup.find_all(lambda tag: tag.name == 'div' and tag.get('class') ==\n",
    "              ['main-content-left'])[0].find_all('a')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## http://www.aaronsw.com\n",
    "http://www.aaronsw.com/weblog/fullarchive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "bowl.text vs bowl.content\n",
    "\n",
    "`bowl.text` is the content of the response in Unicode, and `bowl.content` is the content of the response in bytes. useful when the address refers to such as image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-08T06:16:38.117739Z",
     "start_time": "2019-12-08T06:16:38.056786Z"
    }
   },
   "outputs": [],
   "source": [
    "bowl = requests.get('http://www.aaronsw.com/weblog/fullarchive')\n",
    "soup = BeautifulSoup(bowl.text, 'html.parser')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-family:New York Times; font-size:1em; color:green;\">\n",
    "Extract all the url of posts from soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-08T06:16:52.009473Z",
     "start_time": "2019-12-08T06:16:52.000265Z"
    }
   },
   "outputs": [],
   "source": [
    "soup.body.find_all('a', href = True, limit=10)[1]['href']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-08T06:17:06.064058Z",
     "start_time": "2019-12-08T06:17:06.019767Z"
    }
   },
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(bowl.text, 'html.parser')\n",
    "PostLink = soup.body.find_all('a', href = True)\n",
    "PostLink = [i['href'] for i in PostLink][2:]\n",
    "baseurl = 'http://www.aaronsw.com/weblog/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### single thread approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "startt = time.time()\n",
    "baseurl = 'http://www.aaronsw.com/weblog/'\n",
    "bowls = [ requests.get(baseurl + i) for i in PostLink]\n",
    "time.time()-startt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-family:New York Times; color: red\">\n",
    "Takes around 6 minutes\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-family:New York Times; font-size:1em; color:green;\">\n",
    "It's a good scenario to apply multithreadings\n",
    "    \n",
    "Q: Is the task CPU intensive or I/O intensive? If the answer is I/O intensive, then you can go with threads.\n",
    "\n",
    "https://stackoverflow.com/questions/40894487/python-threading-or-multiprocessing-for-web-crawler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### multithreading "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-16T11:23:35.077525Z",
     "start_time": "2020-01-16T11:23:35.046279Z"
    }
   },
   "outputs": [],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "startt = time.time()\n",
    "pool = ThreadPoolExecutor(6)\n",
    "res = []\n",
    "future = pool.submit(requests.get, [baseurl+i for i in PostLink])\n",
    "time.time()-startt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "startt = time.time()\n",
    "with ThreadPoolExecutor(8) as executor:\n",
    "    bowls = executor.map(requests.get, [baseurl+i for i in PostLink])\n",
    "time.time()-startt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "startt = time.time()\n",
    "with ThreadPoolExecutor(12) as executor:\n",
    "    bowls_2 = executor.map(requests.get, [baseurl+i for i in PostLink])\n",
    "time.time()-startt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### multiprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-16T11:10:21.722814Z",
     "start_time": "2020-01-16T11:10:21.570776Z"
    }
   },
   "outputs": [],
   "source": [
    "from multiprocessing import Pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "startt = time.time()\n",
    "with Pool(6) as p:\n",
    "    bowls = p.map(requests.get, [baseurl+i for i in PostLink])\n",
    "time.time()-startt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### write the result to a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(bowls)):\n",
    "    with open('AaronSwartz/{}.html'.format(i), 'wb') as f:\n",
    "        f.write(BeautifulSoup(bowls[i].text, 'html.parser').encode('utf-8'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  http://arxiv.org"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### by a higher level approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Paper():\n",
    "    \"\"\" A class that holds the information for an Arxiv paper. \"\"\"\n",
    "    def __init__(self,\n",
    "                 number=None,\n",
    "                 title=None,\n",
    "                 auths=None,\n",
    "                 abstract=None,\n",
    "                 fromfile=None):\n",
    "        \"\"\" Initialize a paper with the arxiv number, title, authors, and abstract. \"\"\"\n",
    "\n",
    "        if fromfile is not None:\n",
    "            self.load(fromfile)\n",
    "\n",
    "        else:\n",
    "            self.number = number\n",
    "            self.title = title\n",
    "            if auths is not None:\n",
    "                self.authors = list(auths.values())\n",
    "                self.author_ids = list(auths.keys())\n",
    "                self.author_dict = auths.copy()\n",
    "            else:\n",
    "                self.authors = None\n",
    "                self.author_ids = None\n",
    "                self.author_dict = None\n",
    "\n",
    "            self.abstract = abstract\n",
    "            self.link = u'http://arxiv.org/abs/' + number\n",
    "\n",
    "    def format_line(self, strval, maxlength, pad_left, pad_right):\n",
    "        \"\"\" Function to format a line of a given length.\n",
    "        Used by the __str__ routine.\"\"\"\n",
    "        temp = re.sub(\"(.{\" + \"{:d}\".format(maxlength) + \"})\", u\"\\\\1-\\n\",\n",
    "                      strval.replace('\\n', ''), 0, re.DOTALL).strip()\n",
    "\n",
    "        temp = temp.split('\\n')\n",
    "\n",
    "        temp[-1] = temp[-1] + ''.join([u'\\u0020'] *\n",
    "                                      (maxlength - len(temp[-1])))\n",
    "        if len(temp) > 1:\n",
    "            temp[0] = temp[0][:-1] + temp[0][-1]\n",
    "\n",
    "        return pad_left + (pad_right + '\\n' + pad_left).join(temp) + pad_right\n",
    "\n",
    "    def get_search_string(self):\n",
    "\n",
    "        return '  '.join(\n",
    "            [self.abstract.lower(),\n",
    "             self.title.lower(), self.number] +\n",
    "            [a.lower()\n",
    "             for a in self.author_ids] + [a.lower() for a in self.authors])\n",
    "\n",
    "    def save(self, filename):\n",
    "        with open(filename, \"a\") as f:\n",
    "            json.dump(vars(self), f)\n",
    "\n",
    "    def load(self, filename):\n",
    "        try:\n",
    "            if os.path.exists(filename):\n",
    "                with open(filename, 'r') as f:\n",
    "                    dat = json.load(f)\n",
    "            else:\n",
    "                dat = filename\n",
    "        except TypeError:\n",
    "            dat = filename\n",
    "        for key, val in dat.items():\n",
    "            setattr(self, key, val)\n",
    "\n",
    "    def __eq__(self, paper):\n",
    "        return (self.number == paper.number)\n",
    "\n",
    "    def __ne__(self, paper):\n",
    "        return not self.__eq__(paper)\n",
    "\n",
    "    def __le__(self, paper):\n",
    "        return float(self.number) <= float(paper.number)\n",
    "\n",
    "    def __ge__(self, paper):\n",
    "        return float(self.number) >= float(paper.number)\n",
    "\n",
    "    def __lt__(self, paper):\n",
    "        return float(self.number) < float(paper.number)\n",
    "\n",
    "    def __gt__(self, paper):\n",
    "        return float(self.number) > float(paper.number)\n",
    "\n",
    "    def __str__(self):\n",
    "        \"\"\" Display the paper in a somewhat nice looking way. \"\"\"\n",
    "\n",
    "        maxlen = 80\n",
    "        pad_char = u\"\\u0025\"\n",
    "        newline_char = u\"\\u000A\"\n",
    "        space_char = u\"\\u0020\"\n",
    "        tab_char = space_char + space_char + space_char + space_char\n",
    "        comma_char = u\"\\u002C\"\n",
    "        and_char = u\"\\u0026\"\n",
    "\n",
    "        pad_left = pad_char + pad_char + pad_char + tab_char\n",
    "        pad_right = tab_char + pad_char + pad_char + pad_char\n",
    "\n",
    "        if len(self.authors) == 1:\n",
    "            authstr = self.authors[0]\n",
    "        else:\n",
    "            authstr = (comma_char + space_char).join(self.authors[:-1])\n",
    "            authstr += comma_char + space_char + and_char + space_char + self.authors[\n",
    "                -1]\n",
    "\n",
    "        authstr = self.format_line(authstr, maxlen, pad_left, pad_right)\n",
    "        titlestr = self.format_line(self.title, maxlen, pad_left, pad_right)\n",
    "        linkstr = self.format_line(self.link, maxlen, pad_left, pad_right)\n",
    "        border = ''.join([pad_char] *\n",
    "                         (maxlen + len(pad_left) + len(pad_right)))\n",
    "        blank_line = pad_left + ''.join([space_char] * maxlen) + pad_right\n",
    "\n",
    "\n",
    "        strbody = newline_char + \\\n",
    "                border + newline_char + \\\n",
    "                blank_line  + newline_char + \\\n",
    "                titlestr + newline_char + \\\n",
    "                blank_line  + newline_char + \\\n",
    "                linkstr + newline_char + \\\n",
    "                blank_line  + newline_char + \\\n",
    "                authstr + newline_char + \\\n",
    "                blank_line  + newline_char + \\\n",
    "                border + newline_char + \\\n",
    "                newline_char\n",
    "\n",
    "        # Check for python 2 to convert from unicode\n",
    "        if sys.version_info < (3, ):\n",
    "            strbody = strbody.encode(\"utf8\", \"ignore\")\n",
    "        return strbody"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def authors_list_to_dict(author_list):\n",
    "\n",
    "    authors_dict = {}\n",
    "    for a in author_list:\n",
    "\n",
    "        if '(' in a:\n",
    "            # We have an affiliation\n",
    "            a = a.split('(')[0]\n",
    "            #a = ' ' .join(a.split('(')[0])\n",
    "        temp = a.split()\n",
    "\n",
    "        if len(temp) > 2:\n",
    "            # More than two names, take first and last\n",
    "            name = (temp[0],temp[-1])\n",
    "        elif len(temp) == 1:\n",
    "            # Just one name, probably a spacing error\n",
    "            temp = temp[0].split('.')\n",
    "            name = (temp[0],temp[-1])\n",
    "        else:\n",
    "            # Two names\n",
    "            name = (temp[0],temp[1])\n",
    "\n",
    "        authors_dict[name[1]+'_'+name[0][0].upper()] = ' '.join(temp)\n",
    "    return authors_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_paper_from_url(number):\n",
    "\n",
    "    bowl = requests.get('http://arxiv.org/abs/' + str(number))\n",
    "    soup = bs4.BeautifulSoup(bowl.text, 'html.parser')\n",
    "    title = soup.find_all(\n",
    "        'h1', attrs={'class':\n",
    "                     'title mathjax'})[0].text.split('Title:')[-1].strip()\n",
    "\n",
    "    authors = [\n",
    "        x.strip() for x in soup.find_all('div', attrs={'class': 'authors'})[0].\n",
    "        text.split('Authors:')[-1].split(',')\n",
    "    ]\n",
    "\n",
    "    abstract = soup.find_all(\n",
    "        'blockquote',\n",
    "        attrs={'class':\n",
    "               'abstract mathjax'})[0].text.split('Abstract:')[-1].strip()\n",
    "\n",
    "    return Paper(number, title, authors_list_to_dict(authors), abstract)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "read_paper_from_url('1908.04905')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### by a lower level approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bowl = requests.get('http://arxiv.org/abs/'+ str(1908.04905)) \n",
    "soup = bs4.BeautifulSoup(bowl.text, 'html.parser')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-16T12:41:43.755790Z",
     "start_time": "2020-01-16T12:41:43.748813Z"
    }
   },
   "source": [
    "```html\n",
    "<h1 class=\"title mathjax\"><span class=\"descriptor\">Title:</span>Random walk on a lattice in the presence of obstacles: The short-time transient regime, anomalous diffusion and crowding</h1>\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = bs4.BeautifulSoup(bowl.text, 'html.parser')\n",
    "#soup\n",
    "title = soup.find_all(\n",
    "        'h1', attrs={'class':\n",
    "                     'title mathjax'})[0].text.split('Title:')[-1].strip()\n",
    "title\n",
    "authors = [\n",
    "        x.strip() for x in soup.find_all('div', attrs={'class': 'authors'})[0].\n",
    "        text.split('Authors:')[-1].split(',')\n",
    "    ]\n",
    "authors\n",
    "abstract = soup.find_all('blockquote',attrs={'class':\n",
    "               'abstract mathjax'})[0].text.split('Abstract:')[-1].strip()\n",
    "abstract"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## https://www.gotouniversity.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  `bs4`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bowl = requests.get('https://www.gotouniversity.com/course/index') \n",
    "soup = bs4.BeautifulSoup(bowl.text, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "UniversityName = [i.text for i in soup.find_all('p', attrs={'class': 'university-name'})]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When writing a scraper, it's a good idea to look at the source of the HTML file and familiarize yourself with the structure. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('gotouniversity.html', 'wb') as f:\n",
    "    f.write(soup.encode('utf-8'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-16T12:44:27.763065Z",
     "start_time": "2020-01-16T12:44:27.748417Z"
    }
   },
   "source": [
    "```html\n",
    "<a href=\"https://www.gotouniversity.com/programs/accelerated-bachelors/united-states-of-america/sciences/biochemistry-and-biophysics/loyola-university-chicago/bsms-in-biochemistry\" target=\"_blank\">\n",
    "<span class=\"large-text program-name\" title=\"BSMS in Biochemistry\">BSMS in Biochemistry</span>\n",
    "</a>\n",
    "<script type=\"application/ld+json\">\n",
    "{\n",
    "  \"@context\": \"http://schema.org\",\n",
    "  \"@type\": \"Course\",\n",
    "  \"name\": \"BSMS in Biochemistry\",\n",
    "  \"description\": \"\",\n",
    "  \"provider\": {\n",
    "    \"@type\": \"Organization\",\n",
    "    \"name\": \"Loyola University Chicago\",\n",
    "    \"sameAs\": \"https://www.gotouniversity.com/university/loyola-university-chicago\"\n",
    "  }\n",
    "}\n",
    "</script>\n",
    "<a href=\"/university/loyola-university-chicago\" target=\"_blank\" title=\"University\">\n",
    "<p class=\"university-name\" title=\"Loyola University Chicago\">Loyola University Chicago</p>\n",
    "</a>\n",
    "<p class=\"location-name\" title=\"Chicago Illinois\"> Chicago, Illinois</p>\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://www.gotouniversity.com/course/index'\n",
    "\n",
    "params = {'page': 80}\n",
    "UniversityName = []\n",
    "ProjectName = []\n",
    "for page in range(1, 11):\n",
    "    #update params\n",
    "    params['page'] = page\n",
    "    # requests.post?\n",
    "    soup = BeautifulSoup(requests.post(url, data=params).text, 'html.parser' )\n",
    "    UniversityName.append([a.get_text(strip=True) for a in soup.select('a[title=\"University\"]') ])\n",
    "    ProjectName.append([a.get_text(strip=True) for a in soup.select('span[class=\"large-text program-name\"]')])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### design data format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "UniversityName = sum(UniversityName, [])\n",
    "ProjectName = sum(ProjectName, [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\"ProjectName\": ProjectName, \"UniversityName\": UniversityName}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data)\n",
    "df.to_json('gotoun.json') \n",
    "df.to_excel('gotoun.xlsx', engine='xlsxwriter')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### selenium"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* https://stackoverflow.com/q/51591849\n",
    "  \n",
    "  The only way to do this is to execute the Javascript that handles the click event - you won't do it with a regular GET request.\n",
    "* https://stackoverflow.com/q/31442119"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "driver = webdriver.Chrome(executable_path='/Users/wangmiao/Desktop/chromedriver')\n",
    "# find the element that's name attribute is q (the google search box)\n",
    "driver.get('https://www.gotouniversity.com/course/index')\n",
    "university_name = driver.find_elements_by_class_name(\"university-name\")\n",
    "university_name = [link.text for link in university_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 如果href链接`(<a>)`值是javascript:void(0) 而不是网址，暗示链接不用于href响应页面导航，但使用绑定到链接点击事件的javascript函数来响应用户点击。因此无法driver.get(url)直接打开目标页面，必须单击链接以触发单击事件，该事件将调用javascript函数以导航到目标页面。\n",
    "\n",
    "> 提示需要等待一段时间才能完成浏览器加载javascript并注册javascript函数以链接点击事件。否则点击链接后没有任何反应。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "%%html\n",
    "<div class=\"pagination\"><div aria-live=\"polite\" role=\"status\" style=\"float:left; height:14px; padding:8px\">Showing 1 to 20 of 143981 entries</div><div style=\"float:right;\"><ul class=\"pagination\" id=\"pagin_count\"><li class=\"active\" p=\"1\"><a>1</a></li><li p=\"2\"><a href=\"javascript:void()\" onclick=\"pagingcustom(2);\">2</a></li><li p=\"3\"><a href=\"javascript:void()\" onclick=\"pagingcustom(3);\">3</a></li><li p=\"4\"><a href=\"javascript:void()\" onclick=\"pagingcustom(4);\">4</a></li><li p=\"5\"><a href=\"javascript:void()\" onclick=\"pagingcustom(5);\">5</a></li><li p=\"6\"><a href=\"javascript:void()\" onclick=\"pagingcustom(6);\">6</a></li><li p=\"7\"><a href=\"javascript:void()\" onclick=\"pagingcustom(7);\">7</a></li><li p=\"8\"><a href=\"javascript:void()\" onclick=\"pagingcustom(8);\">8</a></li><li p=\"9\"><a href=\"javascript:void()\" onclick=\"pagingcustom(9);\">9</a></li><li p=\"10\"><a href=\"javascript:void()\" onclick=\"pagingcustom(10);\">10</a></li><li p=\"1\"><a href=\"javascript:void()\" onclick=\"pagingcustom(1);\">Next</a></li></ul></div></div>\n",
    "</div>\n",
    "<script>\n",
    "function fn_advcount(id){\n",
    "    $.ajax({\n",
    "            url: 'https://www.gotouniversity.com/site/advertisement-count',\n",
    "            data: { id : id },\n",
    "            success: function(result){\n",
    "    }});\n",
    "  }\n",
    "</script>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"javascript:void(0)\" means that the link wouldn't work. It'll do nothing. That is why no action is taking place when you click on it.\n",
    "    \n",
    "* https://stackoverflow.com/a/1291950/7583919"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/a/35786344/7583919\n",
    "aElements = driver.find_elements_by_tag_name(\"a\")\n",
    "result = []\n",
    "for name in aElements:\n",
    "    if(name.get_attribute(\"href\") is not None and \"javascript:void()\" in name.get_attribute(\"href\")):\n",
    "        print(\"IM IN HUR\")\n",
    "        \"\"\"\n",
    "        elements = driver.find_elements_by_class_name(\"university-name\")\n",
    "        result.append([link.text for link in elements])\n",
    "        print(result)\n",
    "        \"\"\"\n",
    "        name.click()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://codeday.me/bug/20190123/563610.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://stackoverflow.com/questions/52876136/google-search-next-pages-using-selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "driver = webdriver.Chrome(executable_path='/Users/wangmiao/Desktop/chromedriver')\n",
    "driver.get('https://www.gotouniversity.com/course/index')\n",
    "Page_number = 1\n",
    "Max_page = 10\n",
    "\n",
    "while Page_number <= Max_page:\n",
    "\n",
    "    university_name = WebDriverWait(driver, 20).until(\n",
    "        EC.presence_of_all_elements_located((By.CSS_SELECTOR,\n",
    "                                             '.university-name')))\n",
    "    university_name = [link.text for link in university_name]\n",
    "    print(university_name)\n",
    "    Page_number = Page_number + 1\n",
    "    element = WebDriverWait(driver, 20).until(\n",
    "        EC.element_to_be_clickable((By.XPATH,\n",
    "                                    '//a[text()=\"' + str(Page_number) + '\"]')))\n",
    "    driver.execute_script(\"arguments[0].click();\", element)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## https://www.premierleague.com/players\n",
    "\n",
    "Besides getting the content parsed in html, there is other format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup as soup\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "player_name = ['Bernd Leno', 'Emiliano Martínez', 'Matt Macey', 'Héctor Bellerín']\n",
    "player = {}\n",
    "for i in player_name:\n",
    "    player_page = requests.get(\n",
    "        'https://www.premierleague.com/players/10483/{}/stats'.format(i))\n",
    "    cont = soup(player_page.content, 'lxml')\n",
    "\n",
    "    data = dict(\n",
    "        (k.contents[0].strip(), v.get_text(strip=True)) for k, v in zip(\n",
    "            cont.select('.topStat span.stat, .normalStat span.stat'),\n",
    "            cont.select(\n",
    "                '.topStat span.stat > span, .normalStat span.stat > span')))\n",
    "    player[i] = data\n",
    "\n",
    "pprint(player)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## http://fz.people.com.cn/skygb/sk/index.php/Index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### single thread"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bowl = requests.get('http://fz.people.com.cn/skygb/sk/index.php/Index') \n",
    "soup = bs4.BeautifulSoup(bowl.text, 'html.parser')\n",
    "soup.select(\"span[title]\")[::20][0].get_text()\n",
    "soup.select(\"span[title]\")\n",
    "\n",
    "\n",
    "url = 'http://fz.people.com.cn/skygb/sk/index.php'\n",
    "for page in range(1, 3):\n",
    "    print(params)\n",
    "    bowl = requests.post(url + \"?&p={}\".format(page))\n",
    "    #print(bowl.url)\n",
    "    soup = BeautifulSoup(bowl.text, 'html.parser' )\n",
    "    print([a.get_text(strip=True) for a in soup.select(\"span[title]\")])\n",
    "    print(\"xxx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-16T12:22:55.377117Z",
     "start_time": "2020-01-16T12:22:55.374060Z"
    }
   },
   "source": [
    "### parallel requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'http://fz.people.com.cn/skygb/sk/index.php'\n",
    "startt = time.time()\n",
    "with Pool(6) as p:\n",
    "    bowls = p.map(requests.post,\n",
    "                  [url + \"?&p={}\".format(i) for i in range(1, 600)])\n",
    "time.time() - startt\n",
    "\n",
    "res = [[\n",
    "    x.get_text(strip=True)\n",
    "    for x in BeautifulSoup(response.text, 'html.parser').select(\"span[title]\")\n",
    "] for response in bowls]\n",
    "res = sum(res, [])\n",
    "\n",
    "startt = time.time()\n",
    "with ThreadPoolExecutor(8) as executor:\n",
    "    bowls_1 = executor.map(\n",
    "        requests.post, [url + \"?&p={}\".format(i) for i in range(600, 1582)])\n",
    "time.time() - startt\n",
    "\n",
    "res_2 = [[\n",
    "    x.get_text(strip=True)\n",
    "    for x in BeautifulSoup(response.text, 'html.parser').select(\"span[title]\")\n",
    "] for response in bowls_1]\n",
    "res_2 = sum(res_2, [])\n",
    "final_result = res + res_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {'项目批准号': final_result[0::20]}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "title = [\n",
    "    \"项目类别\", \"学科分类\", \"项目名称\", \"立项时间\", \"项目负责人\", \"专业职务\", \"工作单位\", \"单位类别\", \"所在省区市\",\n",
    "    \"所属系统\"\n",
    "]\n",
    "\n",
    "for i, v in enumerate(title, 1):\n",
    "    df[v] = final_result[i::20]\n",
    "    \n",
    "filter_result = df[~df[\"立项时间\"].str.contains(\"2013\")]\n",
    "filter_result.to_excel(\"classified.xlsx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `bs4`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[A Simple Cheat Sheet for Web Scraping with Python](https://blog.hartleybrody.com/web-scraping-cheat-sheet/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://stackoverflow.com/questions/57767188/python-beautifulsoup-replace-links-with-url-in-string#57767188"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "html=\"\"\"\n",
    "<html><head></head>\n",
    "<body>\n",
    "<a href=\"www.google.com\">foo</a> some text \n",
    "<a href=\"www.bing.com\">bar</a> some <br> text\n",
    "</body></html>\"\"\"\n",
    "\n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "for a_tag in soup.find_all('a'):\n",
    "    a_tag.string = a_tag.get('href')\n",
    "print(soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://www.basketball-reference.com/players/a/abrinal01.html'\n",
    "res = requests.get(url)\n",
    "res.raise_for_status()\n",
    "\n",
    "soup = bs4.BeautifulSoup(res.text, 'html.parser')\n",
    "elems = soup.select('#per_game')\n",
    "\n",
    "table = soup.find(\"table\", { \"id\" : \"per_game\" })\n",
    "table_rows = table.find_all('tr')\n",
    "\n",
    "for tr in table_rows:\n",
    "    td = tr.find_all('td') + tr.find_all('th')\n",
    "    row = [i.text for i in td]\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract from  `xml`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `html` vs `xml`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* `html` is static because it is used to display data.\t\n",
    "* `xml` is dynamic because it is used to transport the data not for displaying the data.\n",
    "*  One other potential advantage to using `xml` is that some processing can be moved client-side as opposed to server-side."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-06T14:17:46.309485Z",
     "start_time": "2020-01-06T14:17:46.306582Z"
    }
   },
   "source": [
    "### Why `xml` and when should I use it?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Here is a common scenario: Data is stored as XML, retrieved by Java, and displayed in HTML. The underlying code remains the same (meaning that a programmer doesn’t have to sit there all day making changes), and the screen doesn’t refresh constantly, annoying the end user. But when there’s a need for new data, it’s there in its current form."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[xml preview](https://codebeautify.org/xmlviewer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## parse by Beautifulsoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-06T14:49:40.089847Z",
     "start_time": "2020-01-06T14:49:28.952934Z"
    }
   },
   "outputs": [],
   "source": [
    "url = \"https://complicatedphenomenon.gitlab.io/atom.xml\"\n",
    "bowl = requests.get(url)\n",
    "soup = BeautifulSoup(bowl.text, \"lxml\")\n",
    "soup.find_all(\"content\")[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## parse by xml.etree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "掌握http协议，熟悉html、dom、xpath等常见的数据抽取技术"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://docs.python.org/3/library/xml.etree.elementtree.html\n",
    "\n",
    "A good tool with an explicit documentation about its API would get your work done soon. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `xml` from local file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-06T12:14:27.149204Z",
     "start_time": "2020-01-06T12:14:27.145667Z"
    }
   },
   "outputs": [],
   "source": [
    "from xml.etree import cElementTree as ET"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We can import this data by reading from a file\n",
    "* Or directly from a string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-family:New York Times; font-size:1.2em; color:red;\">\n",
    "    \n",
    "the `<poll>` element contains a couple of \"attributes\", such as `title` `totalvotes` `name` that give even more information!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-06T12:14:42.661780Z",
     "start_time": "2020-01-06T12:14:42.639921Z"
    }
   },
   "outputs": [],
   "source": [
    "xmlstr = \"\"\"<poll title=\"User Suggested Number of Players\" totalvotes=\"0\" name=\"suggested_numplayers\">\n",
    "<results numplayers=\"3+\"> \n",
    "</results></poll>\n",
    "\"\"\"\n",
    "root = ET.fromstring(xmlstr)\n",
    "root.tag\n",
    "root.attrib\n",
    "root.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-06T12:16:25.020290Z",
     "start_time": "2020-01-06T12:16:25.014106Z"
    }
   },
   "outputs": [],
   "source": [
    "xmlstr = '''<root>\n",
    "<level>\n",
    "  <name>Matthias</name>\n",
    "  <age>23</age>\n",
    "  <gender>Male</gender>\n",
    "</level>\n",
    "<level>\n",
    "  <name>Foo</name>\n",
    "  <age>24</age>\n",
    "  <gender>Male</gender>\n",
    "</level>\n",
    "<level>\n",
    "  <name>Bar</name>\n",
    "  <age>25</age>\n",
    "  <gender>Male</gender>\n",
    "</level>\n",
    "</root>'''\n",
    "\n",
    "root = ET.fromstring(xmlstr)\n",
    "levels = root.findall('level')\n",
    "for level in levels:\n",
    "    name = level.find('name').text\n",
    "    age = level.find('age').text\n",
    "    print(name, age)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-06T14:51:56.420720Z",
     "start_time": "2020-01-06T14:51:56.415549Z"
    }
   },
   "outputs": [],
   "source": [
    "dir(ET)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  `xml` from url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-06T15:04:01.272412Z",
     "start_time": "2020-01-06T15:03:58.179967Z"
    }
   },
   "outputs": [],
   "source": [
    "!wget  $url  -O CP.xml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-06T15:04:52.447523Z",
     "start_time": "2020-01-06T15:04:52.250409Z"
    }
   },
   "outputs": [],
   "source": [
    "tree = ET.parse('CP.xml')\n",
    "root = tree.getroot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-06T15:04:59.228382Z",
     "start_time": "2020-01-06T15:04:59.222431Z"
    }
   },
   "outputs": [],
   "source": [
    "root.tag\n",
    "root.attrib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-06T15:08:20.106398Z",
     "start_time": "2020-01-06T15:08:20.089111Z"
    }
   },
   "outputs": [],
   "source": [
    "for child in root:\n",
    "    print(child.tag, child.attrib)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-06T15:10:49.388474Z",
     "start_time": "2020-01-06T15:10:49.383508Z"
    }
   },
   "outputs": [],
   "source": [
    "#dir(root[0])\n",
    "root[0].tag"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
