{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cp -r /Users/wangmiao/Playground/Crawler/QzoneExporter/1083088454/shuoshuo/ ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%mkdir QQ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "% mv shuos* QQ \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_info = pd.read_json(\"QQ/shuo.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://stackoverflow.com/questions/49505872/read-json-to-pandas-dataframe-valueerror-mixing-dicts-with-non-series-may-lea"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import json data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "onlyfiles = [f for f in listdir(\"QQ\") if isfile(join(\"QQ\", f))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "onlyfiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "# prefer using the glob module, as it does pattern matching and expansion.\n",
    "jsFiles = glob.glob(\"QQ/*.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['QQ/shuoshuo_00360-00399.json',\n",
       " 'QQ/shuoshuo_00280-00319.json',\n",
       " 'QQ/shuoshuo_00200-00239.json',\n",
       " 'QQ/shuoshuo_00040-00079.json',\n",
       " 'QQ/shuoshuo_00320-00359.json',\n",
       " 'QQ/shuoshuo_00240-00279.json',\n",
       " 'QQ/shuoshuo_00120-00159.json',\n",
       " 'QQ/shuoshuo_00160-00199.json',\n",
       " 'QQ/shuoshuo_00080-00119.json',\n",
       " 'QQ/shuoshuo_00400-00427.json',\n",
       " 'QQ/shuoshuo_00000-00039.json']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jsFiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(jsFiles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract the wanted information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shuoshuo = [json.load(open(single_file)) for single_file in jsFiles] \n",
    "message = [pd.DataFrame(i['msglist']) for i in shuoshuo]\n",
    "dfs = [df for df in message]\n",
    "df = pd.concat(dfs, axis=1)\n",
    "print(df.loc[1:100]['content'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Method of writting to a file:\n",
    "```py\n",
    "with open('somefile.txt', 'a') as the_file:\n",
    "    the_file.write('Hello\\n')\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the word cloud\n",
    "messageQ = ''\n",
    "with open('FW.txt', 'a') as the_file:\n",
    "    for i in list(range(len(all_file))):\n",
    "        for j in list(range(len(message[i]))):\n",
    "            messageQ += message[i][j]['content']\n",
    "    the_file.write(messageQ)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Using package jieba](https://www.jianshu.com/p/00ade300bc4e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "from os import path\n",
    "from wordcloud import WordCloud, ImageColorGenerator\n",
    "import jieba.analyse\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.misc import imread\n",
    "\n",
    "class cloud:\n",
    "    def __init__(self, filename, image_filename, font_filename):\n",
    "        self.d = path.dirname(__name__)\n",
    "        content = open(path.join(self.d, filename), 'rb').read()\n",
    "        # 基于TF-IDF算法的关键字抽取, topK返回频率最高的几项, 默认值为20, withWeight\n",
    "        # 为是否返回关键字的权重\n",
    "        tags = jieba.analyse.extract_tags(content, topK=100, withWeight=False)\n",
    "        self.text = \" \".join(tags)\n",
    "        # 需要显示的背景图片\n",
    "        self.img = imread(path.join(self.d, image_filename))\n",
    "        # 指定中文字体, 不然会乱码的\n",
    "        self.wc = WordCloud(font_path=font_filename, background_color='white',\n",
    "                        max_words=300, mask=self.img, max_font_size=40,\n",
    "                        random_state=42)\n",
    "        self.wc.generate(self.text)\n",
    "\n",
    "\n",
    "    def show_wc(self):\n",
    "        '''显示生成的词云图'''\n",
    "        # 让词的颜色和图片的颜色一样\n",
    "        img_color = ImageColorGenerator(self.img)\n",
    "        plt.imshow(self.wc.recolor(color_func=img_color))\n",
    "        plt.axis(\"off\")\n",
    "        plt.show()\n",
    "\n",
    "    def save_wc(self, out_filename):\n",
    "        '''保存到当前目录下'''\n",
    "        self.wc.to_file(path.join(self.d, out_filename))\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    wc = cloud(\"FW.txt\", \"out.png\", \"font.ttc\")\n",
    "    wc.show_wc()\n",
    "    wc.save_wc('output.jpg')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What to with these data.\n",
    "* Aim 1: Observe the Trend \n",
    "* Aim 2: Generate word cloud to have an intuitive impression of the contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "text= open(\"FWNoEmoji.txt\").read()\n",
    "\n",
    "font = 'SimHei.ttf' # download simfang.ttf\n",
    "wc = WordCloud(collocations=False, font_path=font, width=1400, height=1400, margin=2).generate(text.lower())\n",
    "\n",
    "plt.imshow(wc)\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n",
    "\n",
    "wc.to_file('show_Chinese.png')  # 把词云保存下来 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    " \n",
    "text= open(\"TheAdventureofSherlockHolmes.txt\").read()\n",
    "\n",
    "wc = WordCloud(collocations=False,  width=1400, height=1400, margin=2).generate(text.lower())\n",
    "\n",
    "plt.imshow(wc)\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n",
    "\n",
    "wc.to_file('TheAdventureofSherlockHolmes.png')  # 把词云保存下来 \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Depend on a package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jieba\n",
    "jieba.enable_parallel(4)\n",
    "# Setting up parallel processes :4 ,but unable to run on Windows\n",
    "from os import path\n",
    "from scipy.misc import imread\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "# jieba.load_userdict(\"txt\\userdict.txt\")\n",
    "# add userdict by load_userdict()\n",
    "from wordcloud import WordCloud, ImageColorGenerator\n",
    "\n",
    "# get data directory (using getcwd() is needed to support running example in generated IPython notebook)\n",
    "d = path.dirname(__file__) if \"__file__\" in locals() else os.getcwd()\n",
    "\n",
    "stopwords_path = d + '/wc_cn/stopwords_cn_en.txt'\n",
    "# Chinese fonts must be set\n",
    "font_path = d + '/fonts/SourceHanSerif/SourceHanSerifK-Light.otf'\n",
    "\n",
    "# the path to save worldcloud\n",
    "imgname1 = d + '/wc_cn/LuXun.jpg'\n",
    "imgname2 = d + '/wc_cn/LuXun_colored.jpg'\n",
    "# read the mask / color image taken from\n",
    "back_coloring = imread(path.join(d, d + '/wc_cn/LuXun_color.jpg'))\n",
    "\n",
    "# Read the whole text.\n",
    "text = open(path.join(d, d + '/wc_cn/CalltoArms.txt')).read()\n",
    "\n",
    "# if you want use wordCloud,you need it\n",
    "# add userdict by add_word()\n",
    "userdict_list = ['阿Ｑ', '孔乙己', '单四嫂子']\n",
    "\n",
    "\n",
    "# The function for processing text with Jieba\n",
    "def jieba_processing_txt(text):\n",
    "    for word in userdict_list:\n",
    "        jieba.add_word(word)\n",
    "\n",
    "    mywordlist = []\n",
    "    seg_list = jieba.cut(text, cut_all=False)\n",
    "    liststr = \"/ \".join(seg_list)\n",
    "\n",
    "    with open(stopwords_path, encoding='utf-8') as f_stop:\n",
    "        f_stop_text = f_stop.read()\n",
    "        f_stop_seg_list = f_stop_text.splitlines()\n",
    "\n",
    "    for myword in liststr.split('/'):\n",
    "        if not (myword.strip() in f_stop_seg_list) and len(myword.strip()) > 1:\n",
    "            mywordlist.append(myword)\n",
    "    return ' '.join(mywordlist)\n",
    "\n",
    "\n",
    "wc = WordCloud(font_path=font_path, background_color=\"white\", max_words=2000, mask=back_coloring,\n",
    "               max_font_size=100, random_state=42, width=1000, height=860, margin=2,)\n",
    "\n",
    "\n",
    "wc.generate(jieba_processing_txt(text))\n",
    "\n",
    "# create coloring from image\n",
    "image_colors_default = ImageColorGenerator(back_coloring)\n",
    "\n",
    "plt.figure()\n",
    "# recolor wordcloud and show\n",
    "plt.imshow(wc, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n",
    "\n",
    "# save wordcloud\n",
    "wc.to_file(path.join(d, imgname1))\n",
    "\n",
    "# create coloring from image\n",
    "image_colors_byImg = ImageColorGenerator(back_coloring)\n",
    "\n",
    "# show\n",
    "# we could also give color_func=image_colors directly in the constructor\n",
    "plt.imshow(wc.recolor(color_func=image_colors_byImg), interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.figure()\n",
    "plt.imshow(back_coloring, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n",
    "\n",
    "# save wordcloud\n",
    "wc.to_file(path.join(d, imgname2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word cloud figure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud, STOPWORDS\n",
    "from PIL import Image\n",
    "import urllib\n",
    "import requests\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "text= open(\"FWNoEmoji.txt\").read()\n",
    "\n",
    "# the font from github: https://github.com/adobe-fonts\n",
    "font = 'SimHei.ttf' # download simfang.ttf\n",
    "#mask = np.array(Image.open(requests.get('tutu.jpeg', stream=True).raw))\n",
    "mask = np.array(Image.open('tutu.jpeg'))  \n",
    "# This function takes in your text and your mask and generates a wordcloud. \n",
    "def generate_wordcloud(words, mask):\n",
    "    # word_cloud = WordCloud(collocations=False, font_path=font, width=1400, height=1400, margin=2).generate(text.lower())\n",
    "    word_cloud = WordCloud(font_path=font,width = 2*620, height = 2*495, background_color='white', stopwords=STOPWORDS, mask=mask).generate(text)\n",
    "    plt.figure(figsize=(10,8),facecolor = 'white', edgecolor='blue')\n",
    "    \n",
    "    plt.imshow(word_cloud)\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "\n",
    "    word_cloud.to_file('show_Chinese.png')\n",
    "    \n",
    "#Run the following to generate your wordcloud\n",
    "generate_wordcloud(text, mask)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time \n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time.time()\n",
    "datetime.datetime.now()\n",
    "datetime.datetime.now().strftime(\"%y-%m-%d-%H-%M\")\n",
    "print(\"Current year: \", datetime.date.today().strftime(\"%Y\"))\n",
    "print(\"Month of year: \", datetime.date.today().strftime(\"%B\"))\n",
    "print(\"Week number of the year: \", datetime.date.today().strftime(\"%W\"))\n",
    "print(\"Weekday of the week: \", datetime.date.today().strftime(\"%w\"))\n",
    "print(\"Day of year: \", datetime.date.today().strftime(\"%j\"))\n",
    "print(\"Day of the month : \", datetime.date.today().strftime(\"%d\"))\n",
    "print(\"Day of week: \", datetime.date.today().strftime(\"%A\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shuoshuo = []  \n",
    "message = []\n",
    "all_file = sorted(glob.glob('shuoshuo/*.json'))\n",
    "for single_file in all_file:\n",
    "    shuoshuo.append( json.load(open(single_file))) # Read all the json file\n",
    "for i in list(range(len(all_file))):\n",
    "    message.append(pd.DataFrame(shuoshuo[i]['msglist'])) # Draw the main body of each json file and convert it into DataFrame\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "message[1].head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "message[1].info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style = \"font-family: New York Times; color: green\">\n",
    "Now the problem is we need more meaningful description, every piece of information, we hope there is time tag attached, showing when it was created.       \n",
    "</span>"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
