{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cp -r /Users/wangmiao/Playground/Crawler/QzoneExporter/825217399/ ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%less shuoshuo/shuoshuo_00000-00038.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_info = pd.read_json(\"825217399_main_page.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://stackoverflow.com/questions/49505872/read-json-to-pandas-dataframe-valueerror-mixing-dicts-with-non-series-may-lea"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import json data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.datacamp.com/community/tutorials/python-list-comprehension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = []\n",
    "a.append(1) \n",
    "a \n",
    "a.append(2) \n",
    "a "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shuoshuo = []  \n",
    "shuoshuo.append( json.load(open(\"shuoshuo/shuoshuo_00000-00038.json\")))\n",
    "shuoshuo[0]\n",
    "# ! ls shuoshuo/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[How can I do the same operations on many different files?](https://swcarpentry.github.io/python-novice-inflammation/04-files/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(glob.glob('shuoshuo/*.json'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "33"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_file = sorted(glob.glob('shuoshuo/*.json'))\n",
    "type(all_file)\n",
    "len(all_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(range(33))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract the wanted information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Restruct the data \n",
    "shuoshuo = []  \n",
    "message = []\n",
    "content = []\n",
    "all_file = sorted(glob.glob('shuoshuo/*.json'))\n",
    "for single_file in all_file:\n",
    "    shuoshuo.append( json.load(open(single_file))) # Read all the json file\n",
    "for i in list(range(len(all_file))):\n",
    "    message.append(pd.DataFrame(shuoshuo[i]['msglist'])) # Draw the main body of each json file and convert it into DataFrame\n",
    "    \n",
    "#for in in list(range(len(all_file))):\n",
    "#    content.append(message['content'])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Wrangling\n",
    "shuoshuo = []  \n",
    "message = []\n",
    "content = []\n",
    "all_file = sorted(glob.glob('shuoshuo/*.json'))\n",
    "for single_file in all_file:\n",
    "    shuoshuo.append( json.load(open(single_file))) # Read all the json file\n",
    "for i in list(range(len(all_file))):\n",
    "    message.append(shuoshuo[i]['msglist'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#type(message[1]['create_Time2'])\n",
    "# time.ctime(message[1]['created_time'])\n",
    "type(message[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# message[i] contains 39 items, each item is a dict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "message[0][0]['createTime']\n",
    "message[0][1]['createTime']\n",
    "message[0][2]['createTime']\n",
    "len(message[0])\n",
    "message[1][5]['content']\n",
    "message[32][5]['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(message[0][0])\n",
    "message[0][0]['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make the charts\n",
    "for i in list(range(len(all_file))):\n",
    "    for j in list(range(len(message))):\n",
    "        message[i][j]['createTime']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Method of writting to a file:\n",
    "```py\n",
    "with open('somefile.txt', 'a') as the_file:\n",
    "    the_file.write('Hello\\n')\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the word cloud\n",
    "messageQ = ''\n",
    "with open('FW.txt', 'a') as the_file:\n",
    "    for i in list(range(len(all_file))):\n",
    "        for j in list(range(len(message[i]))):\n",
    "            messageQ += message[i][j]['content']\n",
    "    the_file.write(messageQ)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Using package jieba](https://www.jianshu.com/p/00ade300bc4e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "from os import path\n",
    "from wordcloud import WordCloud, ImageColorGenerator\n",
    "import jieba.analyse\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.misc import imread\n",
    "\n",
    "class cloud:\n",
    "    def __init__(self, filename, image_filename, font_filename):\n",
    "        self.d = path.dirname(__name__)\n",
    "        content = open(path.join(self.d, filename), 'rb').read()\n",
    "        # 基于TF-IDF算法的关键字抽取, topK返回频率最高的几项, 默认值为20, withWeight\n",
    "        # 为是否返回关键字的权重\n",
    "        tags = jieba.analyse.extract_tags(content, topK=100, withWeight=False)\n",
    "        self.text = \" \".join(tags)\n",
    "        # 需要显示的背景图片\n",
    "        self.img = imread(path.join(self.d, image_filename))\n",
    "        # 指定中文字体, 不然会乱码的\n",
    "        self.wc = WordCloud(font_path=font_filename, background_color='white',\n",
    "                        max_words=300, mask=self.img, max_font_size=40,\n",
    "                        random_state=42)\n",
    "        self.wc.generate(self.text)\n",
    "\n",
    "\n",
    "    def show_wc(self):\n",
    "        '''显示生成的词云图'''\n",
    "        # 让词的颜色和图片的颜色一样\n",
    "        img_color = ImageColorGenerator(self.img)\n",
    "        plt.imshow(self.wc.recolor(color_func=img_color))\n",
    "        plt.axis(\"off\")\n",
    "        plt.show()\n",
    "\n",
    "    def save_wc(self, out_filename):\n",
    "        '''保存到当前目录下'''\n",
    "        self.wc.to_file(path.join(self.d, out_filename))\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    wc = cloud(\"FW.txt\", \"out.png\", \"font.ttc\")\n",
    "    wc.show_wc()\n",
    "    wc.save_wc('output.jpg')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What to with these data.\n",
    "* Aim 1: Observe the Trend \n",
    "* Aim 2: Generate word cloud to have an intuitive impression of the contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "text= open(\"FWNoEmoji.txt\").read()\n",
    "\n",
    "font = 'SimHei.ttf' # download simfang.ttf\n",
    "wc = WordCloud(collocations=False, font_path=font, width=1400, height=1400, margin=2).generate(text.lower())\n",
    "\n",
    "plt.imshow(wc)\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n",
    "\n",
    "wc.to_file('show_Chinese.png')  # 把词云保存下来 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    " \n",
    "text= open(\"TheAdventureofSherlockHolmes.txt\").read()\n",
    "\n",
    "wc = WordCloud(collocations=False,  width=1400, height=1400, margin=2).generate(text.lower())\n",
    "\n",
    "plt.imshow(wc)\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n",
    "\n",
    "wc.to_file('TheAdventureofSherlockHolmes.png')  # 把词云保存下来 \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Depend on a package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jieba\n",
    "jieba.enable_parallel(4)\n",
    "# Setting up parallel processes :4 ,but unable to run on Windows\n",
    "from os import path\n",
    "from scipy.misc import imread\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "# jieba.load_userdict(\"txt\\userdict.txt\")\n",
    "# add userdict by load_userdict()\n",
    "from wordcloud import WordCloud, ImageColorGenerator\n",
    "\n",
    "# get data directory (using getcwd() is needed to support running example in generated IPython notebook)\n",
    "d = path.dirname(__file__) if \"__file__\" in locals() else os.getcwd()\n",
    "\n",
    "stopwords_path = d + '/wc_cn/stopwords_cn_en.txt'\n",
    "# Chinese fonts must be set\n",
    "font_path = d + '/fonts/SourceHanSerif/SourceHanSerifK-Light.otf'\n",
    "\n",
    "# the path to save worldcloud\n",
    "imgname1 = d + '/wc_cn/LuXun.jpg'\n",
    "imgname2 = d + '/wc_cn/LuXun_colored.jpg'\n",
    "# read the mask / color image taken from\n",
    "back_coloring = imread(path.join(d, d + '/wc_cn/LuXun_color.jpg'))\n",
    "\n",
    "# Read the whole text.\n",
    "text = open(path.join(d, d + '/wc_cn/CalltoArms.txt')).read()\n",
    "\n",
    "# if you want use wordCloud,you need it\n",
    "# add userdict by add_word()\n",
    "userdict_list = ['阿Ｑ', '孔乙己', '单四嫂子']\n",
    "\n",
    "\n",
    "# The function for processing text with Jieba\n",
    "def jieba_processing_txt(text):\n",
    "    for word in userdict_list:\n",
    "        jieba.add_word(word)\n",
    "\n",
    "    mywordlist = []\n",
    "    seg_list = jieba.cut(text, cut_all=False)\n",
    "    liststr = \"/ \".join(seg_list)\n",
    "\n",
    "    with open(stopwords_path, encoding='utf-8') as f_stop:\n",
    "        f_stop_text = f_stop.read()\n",
    "        f_stop_seg_list = f_stop_text.splitlines()\n",
    "\n",
    "    for myword in liststr.split('/'):\n",
    "        if not (myword.strip() in f_stop_seg_list) and len(myword.strip()) > 1:\n",
    "            mywordlist.append(myword)\n",
    "    return ' '.join(mywordlist)\n",
    "\n",
    "\n",
    "wc = WordCloud(font_path=font_path, background_color=\"white\", max_words=2000, mask=back_coloring,\n",
    "               max_font_size=100, random_state=42, width=1000, height=860, margin=2,)\n",
    "\n",
    "\n",
    "wc.generate(jieba_processing_txt(text))\n",
    "\n",
    "# create coloring from image\n",
    "image_colors_default = ImageColorGenerator(back_coloring)\n",
    "\n",
    "plt.figure()\n",
    "# recolor wordcloud and show\n",
    "plt.imshow(wc, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n",
    "\n",
    "# save wordcloud\n",
    "wc.to_file(path.join(d, imgname1))\n",
    "\n",
    "# create coloring from image\n",
    "image_colors_byImg = ImageColorGenerator(back_coloring)\n",
    "\n",
    "# show\n",
    "# we could also give color_func=image_colors directly in the constructor\n",
    "plt.imshow(wc.recolor(color_func=image_colors_byImg), interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.figure()\n",
    "plt.imshow(back_coloring, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n",
    "\n",
    "# save wordcloud\n",
    "wc.to_file(path.join(d, imgname2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word cloud figure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud, STOPWORDS\n",
    "from PIL import Image\n",
    "import urllib\n",
    "import requests\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "text= open(\"FWNoEmoji.txt\").read()\n",
    "\n",
    "# the font from github: https://github.com/adobe-fonts\n",
    "font = 'SimHei.ttf' # download simfang.ttf\n",
    "#mask = np.array(Image.open(requests.get('tutu.jpeg', stream=True).raw))\n",
    "mask = np.array(Image.open('tutu.jpeg'))  \n",
    "# This function takes in your text and your mask and generates a wordcloud. \n",
    "def generate_wordcloud(words, mask):\n",
    "    # word_cloud = WordCloud(collocations=False, font_path=font, width=1400, height=1400, margin=2).generate(text.lower())\n",
    "    word_cloud = WordCloud(font_path=font,width = 2*620, height = 2*495, background_color='white', stopwords=STOPWORDS, mask=mask).generate(text)\n",
    "    plt.figure(figsize=(10,8),facecolor = 'white', edgecolor='blue')\n",
    "    \n",
    "    plt.imshow(word_cloud)\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "\n",
    "    word_cloud.to_file('show_Chinese.png')\n",
    "    \n",
    "#Run the following to generate your wordcloud\n",
    "generate_wordcloud(text, mask)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time \n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time.time()\n",
    "datetime.datetime.now()\n",
    "datetime.datetime.now().strftime(\"%y-%m-%d-%H-%M\")\n",
    "print(\"Current year: \", datetime.date.today().strftime(\"%Y\"))\n",
    "print(\"Month of year: \", datetime.date.today().strftime(\"%B\"))\n",
    "print(\"Week number of the year: \", datetime.date.today().strftime(\"%W\"))\n",
    "print(\"Weekday of the week: \", datetime.date.today().strftime(\"%w\"))\n",
    "print(\"Day of year: \", datetime.date.today().strftime(\"%j\"))\n",
    "print(\"Day of the month : \", datetime.date.today().strftime(\"%d\"))\n",
    "print(\"Day of week: \", datetime.date.today().strftime(\"%A\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "shuoshuo = []  \n",
    "message = []\n",
    "all_file = sorted(glob.glob('shuoshuo/*.json'))\n",
    "for single_file in all_file:\n",
    "    shuoshuo.append( json.load(open(single_file))) # Read all the json file\n",
    "for i in list(range(len(all_file))):\n",
    "    message.append(pd.DataFrame(shuoshuo[i]['msglist'])) # Draw the main body of each json file and convert it into DataFrame\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>certified</th>\n",
       "      <th>cmtnum</th>\n",
       "      <th>commentlist</th>\n",
       "      <th>conlist</th>\n",
       "      <th>content</th>\n",
       "      <th>createTime</th>\n",
       "      <th>created_time</th>\n",
       "      <th>editMask</th>\n",
       "      <th>fwdnum</th>\n",
       "      <th>has_more_con</th>\n",
       "      <th>...</th>\n",
       "      <th>t1_source</th>\n",
       "      <th>t1_subtype</th>\n",
       "      <th>t1_termtype</th>\n",
       "      <th>tid</th>\n",
       "      <th>ugc_right</th>\n",
       "      <th>uin</th>\n",
       "      <th>video</th>\n",
       "      <th>videototal</th>\n",
       "      <th>wbid</th>\n",
       "      <th>wc_flag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>[{'IsPasswordLuckyMoneyCmtRight': '', 'abledel...</td>\n",
       "      <td>[{'con': '其实你一点儿也不难过。', 'type': 2}]</td>\n",
       "      <td>其实你一点儿也不难过。</td>\n",
       "      <td>2018年01月04日</td>\n",
       "      <td>1515027986</td>\n",
       "      <td>4294967294</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>77d12f31127e4d5ab51c0f00</td>\n",
       "      <td>1</td>\n",
       "      <td>825217399</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 50 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   certified  cmtnum                                        commentlist  \\\n",
       "0          0       1  [{'IsPasswordLuckyMoneyCmtRight': '', 'abledel...   \n",
       "\n",
       "                               conlist      content   createTime  \\\n",
       "0  [{'con': '其实你一点儿也不难过。', 'type': 2}]  其实你一点儿也不难过。  2018年01月04日   \n",
       "\n",
       "   created_time    editMask  fwdnum  has_more_con   ...     t1_source  \\\n",
       "0    1515027986  4294967294       0           0.0   ...             1   \n",
       "\n",
       "   t1_subtype  t1_termtype                       tid ugc_right        uin  \\\n",
       "0           2            4  77d12f31127e4d5ab51c0f00         1  825217399   \n",
       "\n",
       "  video  videototal  wbid  wc_flag  \n",
       "0   NaN         NaN     0      NaN  \n",
       "\n",
       "[1 rows x 50 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "message[1].head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 40 entries, 0 to 39\n",
      "Data columns (total 50 columns):\n",
      "certified          40 non-null int64\n",
      "cmtnum             40 non-null int64\n",
      "commentlist        34 non-null object\n",
      "conlist            36 non-null object\n",
      "content            40 non-null object\n",
      "createTime         40 non-null object\n",
      "created_time       40 non-null int64\n",
      "editMask           40 non-null int64\n",
      "fwdnum             40 non-null int64\n",
      "has_more_con       39 non-null float64\n",
      "isEditable         40 non-null int64\n",
      "issigin            40 non-null int64\n",
      "last_fwd           0 non-null float64\n",
      "lbs                40 non-null object\n",
      "name               40 non-null object\n",
      "pic                23 non-null object\n",
      "pic_template       40 non-null object\n",
      "pictotal           23 non-null float64\n",
      "right              40 non-null int64\n",
      "rt_certified       1 non-null float64\n",
      "rt_cmtnum          1 non-null float64\n",
      "rt_con             1 non-null object\n",
      "rt_createTime      1 non-null object\n",
      "rt_fwdnum          1 non-null float64\n",
      "rt_has_more_con    1 non-null float64\n",
      "rt_nosrc           1 non-null float64\n",
      "rt_source          1 non-null float64\n",
      "rt_source_appid    1 non-null object\n",
      "rt_source_name     1 non-null object\n",
      "rt_source_url      1 non-null object\n",
      "rt_sum             40 non-null int64\n",
      "rt_tid             1 non-null object\n",
      "rt_uin             1 non-null float64\n",
      "rt_uinname         1 non-null object\n",
      "rt_wbid            1 non-null float64\n",
      "rtlist             1 non-null object\n",
      "secret             40 non-null int64\n",
      "source_appid       40 non-null object\n",
      "source_name        40 non-null object\n",
      "source_url         40 non-null object\n",
      "t1_source          40 non-null int64\n",
      "t1_subtype         40 non-null int64\n",
      "t1_termtype        40 non-null int64\n",
      "tid                40 non-null object\n",
      "ugc_right          40 non-null int64\n",
      "uin                40 non-null int64\n",
      "video              4 non-null object\n",
      "videototal         4 non-null float64\n",
      "wbid               40 non-null int64\n",
      "wc_flag            1 non-null float64\n",
      "dtypes: float64(13), int64(16), object(21)\n",
      "memory usage: 15.7+ KB\n"
     ]
    }
   ],
   "source": [
    "message[1].info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style = \"font-family: New York Times; color: green\">\n",
    "Now the problem is we need more meaningful description, every piece of information, we hope there is time tag attached, showing when it was created.       \n",
    "</span>"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
